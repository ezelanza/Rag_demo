{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model or OpenAI API, install dependencies\n",
    "!pip install -r requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama from HF\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Enter your local directory you want to store the model in\n",
    "save_path = \"Models/Llama-2-7b-hf\"\n",
    "\n",
    "#Specify the model you want to download from HF\n",
    "hf_model = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "#Instantiate the model and tokenizer (It downloads weights/architecture/parameters)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model, return_dict=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\n",
    "#Save the model and the tokenizer in the local directory specified earlier\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 1: Load PDF\n",
    "\n",
    "loader = PyPDFLoader(\"/home/ec2-user/mnt/Rag_demo/RAG/Data/Dynamic_Resource_Scheduler_for_Distributed_Deep_Learning_Training_in_Kubernetes.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it in chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and vectorize and store\n",
    "\n",
    "persist_directory = 'basic_langchain/chroma_storage'\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "vectordb_loaded = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline,LlamaForCausalLM,LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load_model\n",
    "\n",
    "#from intel_extension_for_transformers.transformers import AutoModelForCausalLM\n",
    "##from transformers import AutoTokenizer\n",
    "#model_dir = \"/home/ec2-user/mnt/Models/Llama-7b-hf-OPTIM\"        \n",
    "#model = AutoModelForCausalLM.from_pretrained(model_dir,use_neural_speed=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model in memory\n",
    "#Model loaded in memory\n",
    "model_dir = \"/home/ec2-user/mnt/Models/llama-2-7b-chat-hf\"        \n",
    "model = LlamaForCausalLM.from_pretrained(model_dir,ignore_mismatched_sizes=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for LLM\n",
    "pipe= pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, \n",
    "                         trust_remote_code=True, max_new_tokens=100, \n",
    "                         repetition_penalty=1.1, model_kwargs={\"max_length\": 1200, \"temperature\": 0.01})\n",
    "        \n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm_pipeline,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       #return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now ask a model for \"DRAGON\". This paper proposes a new method called \"dragon\". This is exactly why RAG helps, let's see what happen if we talk to a foundation model about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "user_question ='Tell me about DRAGON'\n",
    "\n",
    "chain_fm = prompt|llm_pipeline\n",
    "chain_fm.invoke({\"question\": user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET ASK TO A RAG MODEL\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
