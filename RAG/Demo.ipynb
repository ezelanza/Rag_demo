{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE STEPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model / Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model or OpenAI API, install dependencies\n",
    "!pip install -r requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Models/Llama-2-7b-hf/tokenizer_config.json',\n",
       " 'Models/Llama-2-7b-hf/special_tokens_map.json',\n",
       " 'Models/Llama-2-7b-hf/tokenizer.model',\n",
       " 'Models/Llama-2-7b-hf/added_tokens.json',\n",
       " 'Models/Llama-2-7b-hf/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download llama from HF\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Enter your local directory you want to store the model in\n",
    "save_path = \"Models/Llama-2-7b-hf\"\n",
    "\n",
    "#Specify the model you want to download from HF\n",
    "hf_model = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "#Instantiate the model and tokenizer (It downloads weights/architecture/parameters)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model, return_dict=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\n",
    "#Save the model and the tokenizer in the local directory specified earlier\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 1: Load PDF\n",
    "\n",
    "loader = PyPDFLoader(\"/home/ec2-user/mnt/Rag_demo/RAG/Data/Dynamic_Resource_Scheduler_for_Distributed_Deep_Learning_Training_in_Kubernetes.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it in chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['978-1-7281- 8038- 0/20/$31.00 ©2020 IEEE  \\n Dynamic Resource Scheduler for Distributed Deep \\nLearning Training in Kubernetes  \\nMuhammad Fadhriga Bestari  \\nSchool of Electrical Engineering and Informatics , ITB, \\nIndonesia  \\nfadhriga.bestari@gmail.com  \\n Achmad Imam Kistijantoro1,2 \\n1School of Electrical Engineering and Informatics , ITB, \\nIndonesia  \\n2University Center of Excellence on Artificial Intelligence \\nfor Vision, Natural  Language Processing & Big Data \\nAnalytics  (U-CoE AI-VLB), Indonesia  \\nimam@ stei.itb.ac.id\\n \\nAnggrahita Bayu Sasmita  \\nSchool of Electrical Engineering and Informatics , ITB, Indonesia  \\nangga@stei.itb.ac.id  \\n \\n \\nAbstract —Distributed deep learning is a method of machine \\nlearning that is used today due to its many advantages. One of the \\nmany tools used to train distributed deep learning model is Kubeflow, which runs on top of Kubernetes. Kubernetes is a \\ncontainerized application orchestrator that ease the deploy ment \\nprocess of applications. This in turn makes distributed deep \\nlearning training done in Kubeflow easier and manageable. Works \\non a dynamic resource scheduler in Kubernetes for deep learning training have been done before, such as DRAGON that proposed scheduler with autoscaling and gang scheduling capabilities, and \\nOASIS that proposed a utility system with price function. In this \\nwork, we propose to combine DRAGON’s and OASIS’ approach \\nto make a scheduler with weighted autoscaling capabilities and',\n",
       " 'work, we propose to combine DRAGON’s and OASIS’ approach \\nto make a scheduler with weighted autoscaling capabilities and \\nschedu le its jobs with gang scheduling. Some modifications are \\ndone on DRAGON’s autoscaling function. We try to increase the frequency of scaling up function calls and reduce the frequency of \\nscaling down function to make the training process more efficient. \\nWeights are used to determine the priority of each jobs, where jobs \\nwith higher resource requirements are considered more important. Weight of each jobs will influence the autoscaling \\nfunction of the scheduler. Experiment and evaluation done using \\na set of Tensorflow jobs results in an increase of training speed by \\nover 26% in comparison with the default Kubernetes scheduler.  \\nKeywords —Kubernetes, Kubeflow, deep learning job, job \\nscheduling, scale up, scale down, gang scheduling, weighted job  \\nI.  INTRODUCTION  \\nDeep  learning is a subset of machine learning that similarly \\nto humans, uses neurons to learn. These neurons combined \\ncreates a neural network that receive data, process it, and \\ngenerate  a model that can be used to predicts new data based on \\nthe data used in t he training process. Deep learning is more \\nwidely used nowadays due to its generated  models having a \\nmore flexible, scalable and available in comparison with other machine learning methods [ 9]. To generate a model that’s able to predict real data requires',\n",
       " 'a large amount of data to train with. This large data requirement for deep learning increase the model training time considerably. \\nNumber of layers in the neural network and epochs also \\ndetermines the training speed of the model. Complex deep \\nlearning training tends to have a larger amount of data and higher number of layers and epochs, r esulting in a lower training speed.  \\nKubernetes [1] is a platform that manages deployments for \\ncontainerized applications. Kubeflow [3] is a tool developed on \\ntop of Kubernetes that allows distributed deep learning training \\nto be done on Kubernetes with re lative ease. However, \\nKubernetes scheduling functionality are not specifically \\ndesigned to do deep learning training, resulting many problems \\nduring the scheduling process of deploying distributed jobs on \\nKubernetes.  \\nDRAGON [4] is a resource scheduler that  schedules \\ndistributed jobs using gang scheduling and autoscaling that \\nimproves upon the scheduling capabilities of Kubernetes. In this \\nwork we propose to further improve on DRAGON’s approach by weighting each job to determine its priority and integrated t he \\nweighting system with the autoscaling function.  \\nThe scheduler prioritizes jobs with higher resource \\nrequirements to maximize resource utilization without the need \\nto call scale up function. We try to schedule higher priority jobs \\nfirst to reduce the ne ed to call scale down function as well.',\n",
       " 'to call scale up function. We try to schedule higher priority jobs \\nfirst to reduce the ne ed to call scale down function as well. \\nAutoscaling functions cause overhead in the system, so the reduce in number of calls of these functions may improve the \\noverall training speed.   \\nThis paper follows the following structure. In Section 2, we \\ndiscuss related works used as a foundation for our work. Details \\nof our implementation is discussed in Section 3, followed by \\nexperiments and evaluation in Section 4 , and conclusions in \\nsection 5. 2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications (ICAICTA) | 978-1-7281-8038-0/20/$31.00 ©2020 IEEE | DOI: 10.1109/ICAICTA49861.2020.9429033\\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.  II. RELATED WORKS  \\nA. DRAGON \\nDRAGON is an extended controller for managing jo bs on \\nKubernetes [4]. DRAGON uses two approach to increase \\nscheduling efficiency in Kubernetes, which is autoscaling and \\ngang scheduling. Autoscaling is useful because s ystem loading \\nchanges as new jobs enter the system and old jobs finish training. \\nDRAGON utilize scale up function to maximize resource \\nutilization when system loading is low, and utilize scale down \\nfunction to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization,',\n",
       " 'function to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization, \\nit allows other jo bs to be scheduled, possibly increasing the \\noverall resource utilization.  \\nGang scheduling is a scheduling method that DRAGON use \\nto reduce communication overhead in the system. Distributed \\ndeep learning divide training process into two entities, i.e. \\nparam eter server and workers. During the training process, \\nparameter server and workers needs to keep exchanging \\ngradients and weights. Gang scheduling is a scheduling \\nalgorithm that schedule parameter server and workers in the \\nsame node in Kubernetes, reducing  the communication \\noverhead needed for parameter server and workers to \\nsynchronize during training. \\nDRAGON, unlike Kubernetes’ default scheduler, uses a n \\nAdapted First Come First Served (AFCFS)  scheduling \\nalgorithm. AFCFS still queue jobs according to its arrival time, \\nbut different from regular First In First Out (FIFO ) scheduling \\nalgorithm, it schedules the first feasible job in the queue. \\nDRAGON prioritizes jobs that come first, but doesn’t guarantee that jobs will be scheduled in order.  \\nDRAGON’s control flow design is detailed on fig. 1. First, \\nscheduler waits until job enters the system. It queues jobs \\naccording to its arrival time, where jobs that arrive first is \\nprioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned',\n",
       " 'prioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned \\nthreshold is present or not. If it doesn’t, then the scheduler will \\nfind schedulable job using AFCFS and schedule that job using \\ngang scheduling.  However, if it found an urgent job in the queue, \\nit will try to schedule the urgent job. If it failed, the scheduler \\nwill call scale down function to reduce system loading so the urgent job can be scheduled.  \\nSimilar to scale down function, scale up functionality is \\ncalled if the assigned threshold for scale up f unction has been \\nreached. Different from scale down function, scale up function  \\ntimer is not tied to the jobs, rather to how long to system has been in an idle state.  \\nB. OASIS  \\nOASIS [5] is a scheduling algorithm that’s implemented as \\na custom scheduler on Kub ernetes. It utilizes a different \\nscheduling algorithm than the default FIFO approach. OASIS \\ncalculates the optimal job to be scheduled in a given queue. It \\ndoes so by calculating required resource and potential utility of each job.  \\nOASIS calcula te a payof f of each job in the queue using the \\njob’s utility and a price function. OASIS will maximize the payoff of jobs that are scheduled. Jobs that have payoff less than \\nzero will be declined, whereas jobs that have payoff higher than \\nzero can be considered to b e scheduled. \\nThe price function dynamically changes depending on the',\n",
       " 'zero can be considered to b e scheduled. \\nThe price function dynamically changes depending on the \\navailability of resource. The lower the resource availability, the \\nhigher the minimum price of a job to be scheduled. Hence, as the resource availability decreases, the scheduler will pri oritize \\njobs with higher utility.  \\nOASIS proposes three rules regarding price function \\nalgorithm.  \\n1. Price function have a minimum value that allows every \\njob to be scheduled. The lowest value of the job utility \\nneeds to at least the same as the initial price function \\nvalue. This ensure that ev ery job can be scheduled at \\nsome point. \\n2. Price function increases exponentially when allocated \\nresource increases so that the scheduler can deny jobs \\nwith low utility value and save available resource for \\njobs with high ut ility that may come later. \\n3. Price function have a maximum value that denies any \\njob. This ensure that every job will be denied when there \\nis no available resource to be allocated for jobs in the \\nqueue.  \\nIII. P\\nROPOSED METHOD  \\nWe proposed a further development in dynamic resource \\nscheduler by combining both approach of DRAGON and \\nOASIS, where we add a weighting algorithm inspired from \\nOASIS in deep learning jobs and integrate it with autoscaling \\nalgorithm inspired by DRAGON. The  architecture of our \\nproposed method can be seen in fig. 2. \\nAs jobs enter the system, our scheduler will add them into \\nthe queue using priority queueing, where jobs with higher',\n",
       " 'As jobs enter the system, our scheduler will add them into \\nthe queue using priority queueing, where jobs with higher \\nresource requirements. Scheduler then checks for jobs with high \\npriority inside  the queue. If the scheduler doesn’t find a high \\npriority job, it will find the first feasible job using AFCFS \\nscheduling algorithm and use gang scheduling to schedule the job.  \\nDifferently from DRAGON, we proposed to remove the \\nthreshold waiting time for autoscaling and call it as needed. We \\npropose for scale up to be called every time there is no available \\nfeasible job to be scheduled. The overhead of restarting jobs for \\nscaling up, we found, is relatively low in comparison with the \\nincrease of resource u tilization. \\nIf the scheduler found a high priority job in the queue, it will \\ntry to schedule the high priority job first. If it failed, then the \\nscheduler will call scale down function until the high priority job \\ncan be scheduled. However, if the scale dow n function still fails \\nto enable to high priority job to be scheduled, the scheduler will \\ncall scale up function to maximize the remaining resource that’s \\navailable.  \\n \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.   \\nFig. 2.  Proposed system architecture  A. Weighting Algorithm  \\nOur proposed scheduler utilizes two weighting method.',\n",
       " 'Fig. 2.  Proposed system architecture  A. Weighting Algorithm  \\nOur proposed scheduler utilizes two weighting method. \\nFirst, the scheduler calculates the weight of each job using the \\nresource requirements. Scheduler maximizes resource \\nutilization by prioritizing jobs with high resource requirements \\nfirst. By doing so, scheduler can maximize resource utilization \\nwithout the need of calling scale up function.  \\nSecond, scheduler can prioritize jobs by using a given \\npriority value by the user. This weighting method gives the \\nscheduler flexibility to be ab le to adhere to user’s specifications. \\nScheduler will first prioritize jobs with higher priority value \\nassigned by users, then sort the jobs based on the resource \\nrequirements if two or more jobs have the same priority.  \\nThe weighting algorithm used in our proposed method is \\ndisplayed in (1) and  the notation description is detailed  in Table \\nI. \\n 𝑓(𝛾!,𝛾\")=\\t#!\\n!$\\t&\"# (1) \\nTABLE I.  NOTATION DESCRIPTION FOR WEIGHTING ALGORITHM  \\nNotation Description \\n𝛾! Priority of job [1,100] \\n𝛾\" Decay factor of job [0,1] \\n \\nB. Gang Scheduling Algorithm  \\n Gang scheduling is done by  grouping all pod replicas of a \\njob. The algorithm will calculate the aggregate resource \\nrequirements of all replicas for that job. It will then iterate every \\nnode present in the cluster to find a node that h as the resource \\nFig. 1. Control flow and policy of DRAGON [4]',\n",
       " 'node present in the cluster to find a node that h as the resource \\nFig. 1. Control flow and policy of DRAGON [4] \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.  availability to satisfy the job resource requirements. If there \\nwere no node that can satisfy all replica of the job, then the \\nscheduler won’t force the gang scheduling to be done, but rather \\nscheduling the replicas in the available nodes.  \\nC. Scale Down Function \\nScale down function is used to reduce the number of replicas \\nof a job that’s currently being trained. Scale down function will \\niterate every job that’s currently being trained and tries to reduce \\nits number of replicas. If a job has mor e replica than the \\nminimum number of replicas, then the number of replicas for that job will be reduced.  \\nThe iteration will continue until the available resource can \\nsatisfy the resource requirement of the high priority job. If after \\nall jobs’ replicas ha ve been reduced to its minimum values and \\nthe high priority job still can’t be scheduled, then the scale down function will result in a failure state. If it is successful, the \\nscheduler will reduce the selected jobs’ replicas and schedule the high priority  job. \\nD. Scale Up Function \\nScale up function is used to increase the number of replicas \\nof a job that’s currently being trained. Scale up function will',\n",
       " 'D. Scale Up Function \\nScale up function is used to increase the number of replicas \\nof a job that’s currently being trained. Scale up function will \\niterate every job that’s currently being trained and tries to \\nincrease its number of replicas. If a job has  less replica than the \\nmaximum number of replicas, then the number of replicas for that job will be increased . However, if the available resource \\ncan’t satisfy the addition of replicas, then scale up function won’t be called.  \\nIV. E\\nVALUATION  \\nA. Experimental Environment  \\nThe purpose of our experiments is to measure the training \\nspeed of deep learning jobs using our scheduler, DRAGON, and \\nKubeflow’s default tf -operator. We evaluate both DRAGON \\nand default tf- operator to compare our scheduler’s perfo rmance \\nwith a scheduler from the related works section and also the \\ncontroller Kubeflow use today. To evaluate this, we constructed \\na few experiments with different job specifications to evaluate \\nthe schedulers in different job environments. All experiments \\nare done on Google Kubernetes Engine with 3 nodes. Each node \\nhas 8 vCPU and 30GB Memory, totaling to 24 vCPU and 90 GB \\nMemory in the cluster.  \\nEach experiment scenario is done multiple time to avoid \\nfluke result. The job that was used for our experiments are a \\nlinear regression model using MNIST dataset. Specifications of \\njobs that are modified for each experiment mostly revolves',\n",
       " 'linear regression model using MNIST dataset. Specifications of \\njobs that are modified for each experiment mostly revolves \\naround maximum number of workers, initial number of workers, CPU requirement, Memory requirement, and value of job priority.  \\nFor every job in the experiments, we assume its time \\ncriticality to be not critical. Hence, all jobs in the experiments have a time critical value of 0. This in turn, cause the priority \\nand resource requirements of a job to be the only things that \\naffect the weighting algorithm.  B. Autoscaling and Gang Scheduling Evaluation \\nWe conducted 3 experiments to evaluate the effects of \\nautoscaling and gang scheduling on the training speed of deep learning jobs. Jobs specifications for each experiment is detailed  \\non Table II, Table III, and Table IV.  These experiments are done \\nto compare the training speed of our scheduler,  DRAGON, and \\ntf-operator in three different scenarios.  \\nTABLE II.  JOB SPESIFICATION FOR SCENARIO 1 \\nCategory  Job 1 Job 2 Job 3 Job 4 \\nMaximum worker s  4 6 6 6 \\nMinimum worker s  1 2 1 2 \\nInitial worker replica s 2 2 2 2 \\nCPU requirement  1 2 2 3 \\nMemory requirement  2 Gi 4 Gi 4 Gi 6 Gi \\nPriority 1 1 1 100 \\nTABLE III.  JOB SPESIFICATION FOR SCENARIO 2 \\nCategory  Job 1 Job 2 Job 3 Job 4 \\nMaximum worker s  4 6 6 6 \\nMinimum worker s  1 2 1 2 \\nInitial worker replica s 2 2 2 2 \\nCPU requirement  1 1 1 1 \\nMemory requirement  2 Gi 2 Gi 2 Gi 2 Gi \\nPriority 1 1 1 100 \\nTABLE IV.  JOB SPESIFICATION FOR SCENARIO 3',\n",
       " 'CPU requirement  1 1 1 1 \\nMemory requirement  2 Gi 2 Gi 2 Gi 2 Gi \\nPriority 1 1 1 100 \\nTABLE IV.  JOB SPESIFICATION FOR SCENARIO 3 \\nCategory  Job 1 Job 2 Job 3 Job 4 \\nMaximum worker s  4 6 6 6 \\nMinimum worker s  1 2 1 2 \\nInitial worker replica s 2 2 2 2 \\nCPU requirement  3 3 3 3 \\nMemory requirement  6 Gi 6 Gi 6 Gi 6 Gi \\nPriority 1 1 1 100 \\n \\nC. Weighting Function Evaluation  \\nWe conducted 1 experiment to evaluate the effects of \\nweighting function on the training speed of deep learning jobs. \\nJobs specifications for the experiment is detailed  on Table V.  \\nThis experiment is done to compare schedulers with and without \\nweighting  functionality.\\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.    \\nTABLE V.  JOB SPESIFICATION FOR SCENARIO 4 \\nJob Maximum \\nWorker s Minimum \\nWorker s  Initial \\nWorker s CPU \\nrequirement  Memory  \\nrequirement  Priority \\n1 4 2 4 2 4 1 \\n2 4 2 4 2 4 1 \\n3 4 1 2 2 4 1 \\n4 4 1 2 3 6 1 \\n5 4 2 3 2 4 1 \\n6 4 2 2 2 4 1 \\n7 6 2 2 2 4 1 \\n8 6 1 1 2 4 1 \\n9 4 2 2 2 4 1 \\n10 4 2 2 4 8 1 \\n11 4 3 3 2 4 1 \\n12 4 3 3 2 4 1 \\nD. Model Accuracy Evaluation \\nLastly, we also conducted an experiment to evaluate the \\nmodel generated from distributed deep learning job. Job \\nspecifications for the experiment is detailed on Table VI.  In this \\nexperiment we compare models generated using distributed  \\ndeep learning job and centralized deep learning job.',\n",
       " 'experiment we compare models generated using distributed  \\ndeep learning job and centralized deep learning job.  \\nTABLE VI.  JOB SPESIFICATION FOR SCENARIO 5 \\nVariable  Value  \\nBatch size 100 \\nLearning rate 0.05 \\nGlobal  step 100000 \\n \\nE. Evaluation Result  \\nThe result of autoscaling and gang scheduling, weighting \\nfunction, and model accuracy evaluations are displayed on fig. 3, \\nTable VII, and Table VIII respectively.  \\nEvaluation result of each  scenario shows that for autoscaling \\nand gang scheduling evaluation, on the first scenario, our \\nscheduler resulted in an increase of training speed up to 5.06%. \\nOn the second scenario, our scheduler resulted in an increase of \\ntraining speed up to 35.05%. O n the third scenario, our scheduler \\nresulted in an increase of training speed up to 22.38%.  \\nOn the first experiment, the difference of training speed \\nbetween the three schedulers are not that noticeable. This is \\ncaused due to job 4, the last job in the queue, having a relatively \\nhigh resource requirement. Job 4 most likely can’t be scheduled \\nusing gang scheduling, as other jobs have utilized most of the \\navailable resource. Higher resource requirements also hinder the \\nscale up function, as the scheduler may not be able to scale up \\nfunction successfully even though there are still some available \\nresource left.  On the second experiment, we observed the highest speed \\nincrease between our scheduler, DRAGON, and tf -operator.',\n",
       " 'resource left.  On the second experiment, we observed the highest speed \\nincrease between our scheduler, DRAGON, and tf -operator. \\nLow resource requirement for each job enables the scheduler to \\nmaximally utilize the scale up function to increase the resource \\nutilization when training. Low resource requirements for each \\njob also increase the probability of job replicas to be placed in \\nthe same node, since each node can schedule a handful number \\nof replicas. Unsurprisingly, this experiment also resulted in the \\nfastest training speed for every scheduler in comparison with experiment one and three.  \\nOn the third experiment, even though we have a relatively \\nhigh-speed  increase between our scheduler, DRAGON, and tf-\\noperator, the third experiment resulted in the lowest training \\nspeed for every scheduler in comparison with the previous experiments. This experiment shows that having jobs with high \\nresource requirements will not always improves the training \\nspeed. The communication overhead between parameter server \\nand workers will be higher, resulting an overall slower training \\nspeed.  \\n \\nFig. 3.  Evaluation result for experiment 1, 2, and 3. Lower value is better.  \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.  Evaluation result of the fourth experiment shows that for \\nweighting function evaluation, scheduler with weighting',\n",
       " 'weighting function evaluation, scheduler with weighting \\nfunctionality resulted in an increase of training speed up to \\n6.74% in comparison with a scheduler that has no weighting \\nfunctionality. This shows that prioritizing jobs that has higher \\nresource requirements first results in a faster training speed. This \\nis caused due to schedulers with weighting functionality can \\nmaximize available resource without calling scale up function that case a little bit of overhead in the system.  Evaluation result of the fifth experiment shows that for \\nmodel accuracy, our scheduler still falls behind centralized deep \\nlearning training. The maximum accuracy our scheduler can \\nachieve is merely a 70%. Also, the accuracy seems to be \\nfluctuating and our scheduler still unable to generate a consistent \\nmodel. Th e tradeoff between training speed and model accuracy \\nneeds to be heavily considered, and should be addressed in \\nfurther development for our work\\nTABLE VII.  EVALUATION RESULT FOR EXPERIMENT  4 \\ni Scheduler without \\nWeighting (s) Scheduler with Weighting \\n(s) Speed Increase (%) \\n1 654 564 13.76 \\n2 637 670 -4.93 \\n3 667 680 -1.95 \\n4 671 511 23.85 \\n5 675 673 2.96 \\nMean  660.8 ± 15.47 619.6 ± 77.34 6.74 ± 11.92 \\nTABLE VIII.  EVALUATION RESULT FOR EXPERIMENT  5 \\ni Distributed (%) Centralized (%)  1 worker 2 workers 3 workers 4 workers \\n1 32 51 39 53 91 \\n2 40 28 60 70 90 \\n3 24 62 52 52 90 \\n4 15 57 42 51 91 \\n5 31 38 38 40 92 \\nMean 28.4 ± 9.3 47.2 ± 14 46.2 ± 9.5 53.2 ± 11 90.8 ± 0.8',\n",
       " '1 32 51 39 53 91 \\n2 40 28 60 70 90 \\n3 24 62 52 52 90 \\n4 15 57 42 51 91 \\n5 31 38 38 40 92 \\nMean 28.4 ± 9.3 47.2 ± 14 46.2 ± 9.5 53.2 ± 11 90.8 ± 0.8 \\nV. CONCLUSION  \\nOur proposed method of combining DRAGON’s and \\nOASIS’ approach by creating a scheduler with weighting, \\nautoscaling, and gang scheduling capabilities resulted in an \\nincrease of speed for deep learning training jobs. Evaluation \\ndone using MNIST dataset to each functionality shows that an \\nincrease of up to 26.56% is achieved due to the autoscaling and \\ngang scheduling functionalities, while the weighting algorithm \\nalso resulted in an increase of training speed up to 6.23%.  \\nFurther improvement to our work should be focused on \\nimproving weighting algorithm to also consider the estimation \\nof training time for each job. Improvement on the generated \\nmodel should also be done, so that our scheduler can generate \\nmodels with comparable quality with models generated using centralized deep learning. \\nA\\nCKNOWLEDGEMENT  \\nWe are grateful for receiving funding for publication from \\nthe P3MI program at Institut Teknologi Bandung.  REFERENCES  \\n[1] Kubernetes. (2019). Accessed on 8 Oktober, 2019, from  \\nhttps://www.kubernetes.io/ . \\n[2] Docker. (2019). Accessed on 8 Oktober, 2019, from \\nhttps://www.docker.com/ . \\n[3] Kubeflow. (2019). Accessed on 12 Oktober, 2019, dari https://www.kubeflow.org/ . \\n[4] Lin, C., Yeh, T., dan Chou, J. (2019). DRAGON: A Dynamic Scheduling',\n",
       " '[4] Lin, C., Yeh, T., dan Chou, J. (2019). DRAGON: A Dynamic Scheduling \\nand Scaling Controller for Managing Distributed Deep Learning Jobs in Kubernetes Cluster . DOI:10.5220/00077007605690577.  \\n[5] Bao, Y., Peng, Y. , Wu, C., dan Li, Z. (2018). Online Job Scheduling in \\nDistributed Machine Learning Clusters. CoRR, abs/1801.00936.  \\n[6] Rodriguez, M. dan Buyya, R. (2018). Containers Orchestration with Cost -\\nEfficient Autoscaling in Cloud Computing Environments . CoRR, \\nabs/1812. 00300.  \\n[7] Jeon, M., Venkataraman, S., Phanishayee, A., Qian, J., Xiao, W., dan Yang, F. (2018). Multi -tenant GPU Clusters for Deep Learning \\nWorkloads: Analysis and Implications.  Microsoft Research Technical \\nReport.  \\n[8] Amaral, M., Polo, J., Carrera, D., Seelam, S., dan Steinder, M. (2017). \\nTopology- Aware GPU Scheduling for Learning Workloads in Cloud \\nEnvironments . DOI:10.1145/ 3126908.3126933 . \\n[9] Peteiro -Barral, D. dan Guijarro -Bernidas, B. (2013). A Survey of Methods \\nfor Distributed Machine Learning. DOI:10.1007/s13748 -012-0035- 5.\\n \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Embed and vectorize and store\n",
    "\n",
    "persist_directory = 'basic_langchain/chroma_storage'\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "vectordb_loaded = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE : Run the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline,LlamaForCausalLM,LlamaTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.45it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load model in memory\n",
    "#Model loaded in memory\n",
    "model_dir = \"/home/ec2-user/mnt/Models/llama-2-7b-chat-hf\"        \n",
    "model = LlamaForCausalLM.from_pretrained(model_dir,ignore_mismatched_sizes=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for LLM\n",
    "pipe= pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, \n",
    "                         trust_remote_code=True, max_new_tokens=100, \n",
    "                         repetition_penalty=1.1, model_kwargs={\"max_length\": 1200, \"temperature\": 0.01})\n",
    "        \n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm_pipeline,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       #return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now ask a model for \"DRAGON\". This paper proposes a new method called \"dragon\". This is exactly why RAG helps, let's see what happen if we talk to a foundation model about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dragon is a popular fantasy creature that has been depicted in various forms of media throughout history. In modern times, dragons are often portrayed as powerful, fire-breathing reptilian creatures with wings, scales, and claws. They are sometimes depicted as having magical powers or abilities, such as the ability to fly, breathe fire, or cast spells. Dragons are also often associated with hoarding treasure and are frequently featured in'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "fm_prompt = PromptTemplate.from_template(fm_template)\n",
    "user_question ='Tell me about DRAGON'\n",
    "\n",
    "chain_fm = fm_prompt|llm_pipeline\n",
    "chain_fm.invoke({\"question\": user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about DRAGON',\n",
       " 'result': \"DRAGON is a scheduler designed by the authors of this paper to reduce resource utilization when system loading is high. Unlike Kubernetes' default scheduler, which uses a First In First Out (FIFO) algorithm, DRAGON uses an Adapted First Come First Served (AFCFS) algorithm. This means that DRAGON prioritizes jobs that come first, but doesn't guarantee that jobs will be scheduled in order. Additionally, D\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LET ASK TO A RAG MODEL\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Optimized models like : Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/ec2-user/mnt/Models/llama_cpp/llama-2-7b-chat.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/ec2-user/mnt/Models/llama_cpp/llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    #callback_manager=callback_manager,\n",
    "    n_ctx=2048  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       #return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     120.26 ms\n",
      "llama_print_timings:      sample time =      19.70 ms /   100 runs   (    0.20 ms per token,  5076.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20354.25 ms /  1276 tokens (   15.95 ms per token,    62.69 tokens per second)\n",
      "llama_print_timings:        eval time =    4354.45 ms /    99 runs   (   43.98 ms per token,    22.74 tokens per second)\n",
      "llama_print_timings:       total time =   25078.74 ms /  1375 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about DRAGON',\n",
       " 'result': 'DRAGON prioritizes jobs that come first, but doesn’t guarantee that jobs will be scheduled in order. This is because DRAGON uses a Adapted First Come First Served (AFCFS) scheduling algorithm instead of Kubernetes’ default scheduler, which uses a regular First In First Out (FIFO) algorithm. AFCFS still queues jobs according to their arrival time, but prioritizes the first feasible job in the queue. This'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LET ASK TO A RAG MODEL\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
