{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize/Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/ec2-user/miniconda3/lib/python3.11/site-packages (0.1.9)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (2.0.27)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (0.0.24)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (0.1.26)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (0.1.8)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (2.6.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/miniconda3/lib/python3.11/site-packages (4.38.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: sentence-transformers in /home/ec2-user/miniconda3/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.38.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/ec2-user/miniconda3/lib/python3.11/site-packages (1.7.4)\n",
      "Requirement already satisfied: PyPDF2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (3.0.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/miniconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/ec2-user/miniconda3/lib/python3.11/site-packages (4.25.3)\n",
      "Requirement already satisfied: intel_extension_for_transformers in /home/ec2-user/miniconda3/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from intel_extension_for_transformers) (23.2)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from intel_extension_for_transformers) (1.26.4)\n",
      "Requirement already satisfied: schema in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from intel_extension_for_transformers) (0.7.5)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from intel_extension_for_transformers) (6.0.1)\n",
      "Requirement already satisfied: neural-compressor in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from intel_extension_for_transformers) (2.4.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from intel_extension_for_transformers) (4.38.1)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (1.2.14)\n",
      "Requirement already satisfied: opencv-python-headless in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (4.9.0.80)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (2.2.1)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (10.2.0)\n",
      "Requirement already satisfied: prettytable in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (3.10.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (9.0.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: pycocotools in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from neural-compressor->intel_extension_for_transformers) (2.0.7)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from schema->intel_extension_for_transformers) (21.6.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->intel_extension_for_transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->intel_extension_for_transformers) (0.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->intel_extension_for_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->intel_extension_for_transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->intel_extension_for_transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->intel_extension_for_transformers) (4.65.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from deprecated>=1.2.13->neural-compressor->intel_extension_for_transformers) (1.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->intel_extension_for_transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->intel_extension_for_transformers) (4.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->neural-compressor->intel_extension_for_transformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->neural-compressor->intel_extension_for_transformers) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->neural-compressor->intel_extension_for_transformers) (2024.1)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from prettytable->neural-compressor->intel_extension_for_transformers) (0.2.13)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pycocotools->neural-compressor->intel_extension_for_transformers) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->neural-compressor->intel_extension_for_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->neural-compressor->intel_extension_for_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->neural-compressor->intel_extension_for_transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->neural-compressor->intel_extension_for_transformers) (2024.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from scikit-learn->neural-compressor->intel_extension_for_transformers) (3.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel_extension_for_transformers) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->neural-compressor->intel_extension_for_transformers) (1.16.0)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/miniconda3/lib/python3.11/site-packages (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (2.2.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: datasets in /home/ec2-user/miniconda3/lib/python3.11/site-packages (2.17.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting peft\n",
      "  Using cached peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (2.2.1)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (4.38.1)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (0.27.2)\n",
      "Requirement already satisfied: safetensors in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Using cached peft-0.8.2-py3-none-any.whl (183 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.8.2\n"
     ]
    }
   ],
   "source": [
    "# Download model or OpenAI API, install dependencies\n",
    "!pip install -r requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama from HF\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Enter your local directory you want to store the model in\n",
    "save_path = \"Models/Llama-2-7b-hf\"\n",
    "\n",
    "#Specify the model you want to download from HF\n",
    "hf_model = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "#Instantiate the model and tokenizer (It downloads weights/architecture/parameters)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model, return_dict=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\n",
    "#Save the model and the tokenizer in the local directory specified earlier\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 1: Load PDF\n",
    "\n",
    "loader = PyPDFLoader(\"/home/ec2-user/mnt/Rag_demo/RAG/Data/Dynamic_Resource_Scheduler_for_Distributed_Deep_Learning_Training_in_Kubernetes.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it in chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['978-1-7281- 8038- 0/20/$31.00 ©2020 IEEE  \\n Dynamic Resource Scheduler for Distributed Deep \\nLearning Training in Kubernetes  \\nMuhammad Fadhriga Bestari  \\nSchool of Electrical Engineering and Informatics , ITB, \\nIndonesia  \\nfadhriga.bestari@gmail.com  \\n Achmad Imam Kistijantoro1,2 \\n1School of Electrical Engineering and Informatics , ITB, \\nIndonesia  \\n2University Center of Excellence on Artificial Intelligence \\nfor Vision, Natural  Language Processing & Big Data \\nAnalytics  (U-CoE AI-VLB), Indonesia  \\nimam@ stei.itb.ac.id\\n \\nAnggrahita Bayu Sasmita  \\nSchool of Electrical Engineering and Informatics , ITB, Indonesia  \\nangga@stei.itb.ac.id  \\n \\n \\nAbstract —Distributed deep learning is a method of machine \\nlearning that is used today due to its many advantages. One of the \\nmany tools used to train distributed deep learning model is Kubeflow, which runs on top of Kubernetes. Kubernetes is a \\ncontainerized application orchestrator that ease the deploy ment \\nprocess of applications. This in turn makes distributed deep \\nlearning training done in Kubeflow easier and manageable. Works \\non a dynamic resource scheduler in Kubernetes for deep learning training have been done before, such as DRAGON that proposed scheduler with autoscaling and gang scheduling capabilities, and \\nOASIS that proposed a utility system with price function. In this \\nwork, we propose to combine DRAGON’s and OASIS’ approach \\nto make a scheduler with weighted autoscaling capabilities and',\n",
       " 'work, we propose to combine DRAGON’s and OASIS’ approach \\nto make a scheduler with weighted autoscaling capabilities and \\nschedu le its jobs with gang scheduling. Some modifications are \\ndone on DRAGON’s autoscaling function. We try to increase the frequency of scaling up function calls and reduce the frequency of \\nscaling down function to make the training process more efficient. \\nWeights are used to determine the priority of each jobs, where jobs \\nwith higher resource requirements are considered more important. Weight of each jobs will influence the autoscaling \\nfunction of the scheduler. Experiment and evaluation done using \\na set of Tensorflow jobs results in an increase of training speed by \\nover 26% in comparison with the default Kubernetes scheduler.  \\nKeywords —Kubernetes, Kubeflow, deep learning job, job \\nscheduling, scale up, scale down, gang scheduling, weighted job  \\nI.  INTRODUCTION  \\nDeep  learning is a subset of machine learning that similarly \\nto humans, uses neurons to learn. These neurons combined \\ncreates a neural network that receive data, process it, and \\ngenerate  a model that can be used to predicts new data based on \\nthe data used in t he training process. Deep learning is more \\nwidely used nowadays due to its generated  models having a \\nmore flexible, scalable and available in comparison with other machine learning methods [ 9]. To generate a model that’s able to predict real data requires',\n",
       " 'a large amount of data to train with. This large data requirement for deep learning increase the model training time considerably. \\nNumber of layers in the neural network and epochs also \\ndetermines the training speed of the model. Complex deep \\nlearning training tends to have a larger amount of data and higher number of layers and epochs, r esulting in a lower training speed.  \\nKubernetes [1] is a platform that manages deployments for \\ncontainerized applications. Kubeflow [3] is a tool developed on \\ntop of Kubernetes that allows distributed deep learning training \\nto be done on Kubernetes with re lative ease. However, \\nKubernetes scheduling functionality are not specifically \\ndesigned to do deep learning training, resulting many problems \\nduring the scheduling process of deploying distributed jobs on \\nKubernetes.  \\nDRAGON [4] is a resource scheduler that  schedules \\ndistributed jobs using gang scheduling and autoscaling that \\nimproves upon the scheduling capabilities of Kubernetes. In this \\nwork we propose to further improve on DRAGON’s approach by weighting each job to determine its priority and integrated t he \\nweighting system with the autoscaling function.  \\nThe scheduler prioritizes jobs with higher resource \\nrequirements to maximize resource utilization without the need \\nto call scale up function. We try to schedule higher priority jobs \\nfirst to reduce the ne ed to call scale down function as well.',\n",
       " 'to call scale up function. We try to schedule higher priority jobs \\nfirst to reduce the ne ed to call scale down function as well. \\nAutoscaling functions cause overhead in the system, so the reduce in number of calls of these functions may improve the \\noverall training speed.   \\nThis paper follows the following structure. In Section 2, we \\ndiscuss related works used as a foundation for our work. Details \\nof our implementation is discussed in Section 3, followed by \\nexperiments and evaluation in Section 4 , and conclusions in \\nsection 5. 2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications (ICAICTA) | 978-1-7281-8038-0/20/$31.00 ©2020 IEEE | DOI: 10.1109/ICAICTA49861.2020.9429033\\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.  II. RELATED WORKS  \\nA. DRAGON \\nDRAGON is an extended controller for managing jo bs on \\nKubernetes [4]. DRAGON uses two approach to increase \\nscheduling efficiency in Kubernetes, which is autoscaling and \\ngang scheduling. Autoscaling is useful because s ystem loading \\nchanges as new jobs enter the system and old jobs finish training. \\nDRAGON utilize scale up function to maximize resource \\nutilization when system loading is low, and utilize scale down \\nfunction to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization,',\n",
       " 'function to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization, \\nit allows other jo bs to be scheduled, possibly increasing the \\noverall resource utilization.  \\nGang scheduling is a scheduling method that DRAGON use \\nto reduce communication overhead in the system. Distributed \\ndeep learning divide training process into two entities, i.e. \\nparam eter server and workers. During the training process, \\nparameter server and workers needs to keep exchanging \\ngradients and weights. Gang scheduling is a scheduling \\nalgorithm that schedule parameter server and workers in the \\nsame node in Kubernetes, reducing  the communication \\noverhead needed for parameter server and workers to \\nsynchronize during training. \\nDRAGON, unlike Kubernetes’ default scheduler, uses a n \\nAdapted First Come First Served (AFCFS)  scheduling \\nalgorithm. AFCFS still queue jobs according to its arrival time, \\nbut different from regular First In First Out (FIFO ) scheduling \\nalgorithm, it schedules the first feasible job in the queue. \\nDRAGON prioritizes jobs that come first, but doesn’t guarantee that jobs will be scheduled in order.  \\nDRAGON’s control flow design is detailed on fig. 1. First, \\nscheduler waits until job enters the system. It queues jobs \\naccording to its arrival time, where jobs that arrive first is \\nprioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned',\n",
       " 'prioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned \\nthreshold is present or not. If it doesn’t, then the scheduler will \\nfind schedulable job using AFCFS and schedule that job using \\ngang scheduling.  However, if it found an urgent job in the queue, \\nit will try to schedule the urgent job. If it failed, the scheduler \\nwill call scale down function to reduce system loading so the urgent job can be scheduled.  \\nSimilar to scale down function, scale up functionality is \\ncalled if the assigned threshold for scale up f unction has been \\nreached. Different from scale down function, scale up function  \\ntimer is not tied to the jobs, rather to how long to system has been in an idle state.  \\nB. OASIS  \\nOASIS [5] is a scheduling algorithm that’s implemented as \\na custom scheduler on Kub ernetes. It utilizes a different \\nscheduling algorithm than the default FIFO approach. OASIS \\ncalculates the optimal job to be scheduled in a given queue. It \\ndoes so by calculating required resource and potential utility of each job.  \\nOASIS calcula te a payof f of each job in the queue using the \\njob’s utility and a price function. OASIS will maximize the payoff of jobs that are scheduled. Jobs that have payoff less than \\nzero will be declined, whereas jobs that have payoff higher than \\nzero can be considered to b e scheduled. \\nThe price function dynamically changes depending on the',\n",
       " 'zero can be considered to b e scheduled. \\nThe price function dynamically changes depending on the \\navailability of resource. The lower the resource availability, the \\nhigher the minimum price of a job to be scheduled. Hence, as the resource availability decreases, the scheduler will pri oritize \\njobs with higher utility.  \\nOASIS proposes three rules regarding price function \\nalgorithm.  \\n1. Price function have a minimum value that allows every \\njob to be scheduled. The lowest value of the job utility \\nneeds to at least the same as the initial price function \\nvalue. This ensure that ev ery job can be scheduled at \\nsome point. \\n2. Price function increases exponentially when allocated \\nresource increases so that the scheduler can deny jobs \\nwith low utility value and save available resource for \\njobs with high ut ility that may come later. \\n3. Price function have a maximum value that denies any \\njob. This ensure that every job will be denied when there \\nis no available resource to be allocated for jobs in the \\nqueue.  \\nIII. P\\nROPOSED METHOD  \\nWe proposed a further development in dynamic resource \\nscheduler by combining both approach of DRAGON and \\nOASIS, where we add a weighting algorithm inspired from \\nOASIS in deep learning jobs and integrate it with autoscaling \\nalgorithm inspired by DRAGON. The  architecture of our \\nproposed method can be seen in fig. 2. \\nAs jobs enter the system, our scheduler will add them into \\nthe queue using priority queueing, where jobs with higher',\n",
       " 'As jobs enter the system, our scheduler will add them into \\nthe queue using priority queueing, where jobs with higher \\nresource requirements. Scheduler then checks for jobs with high \\npriority inside  the queue. If the scheduler doesn’t find a high \\npriority job, it will find the first feasible job using AFCFS \\nscheduling algorithm and use gang scheduling to schedule the job.  \\nDifferently from DRAGON, we proposed to remove the \\nthreshold waiting time for autoscaling and call it as needed. We \\npropose for scale up to be called every time there is no available \\nfeasible job to be scheduled. The overhead of restarting jobs for \\nscaling up, we found, is relatively low in comparison with the \\nincrease of resource u tilization. \\nIf the scheduler found a high priority job in the queue, it will \\ntry to schedule the high priority job first. If it failed, then the \\nscheduler will call scale down function until the high priority job \\ncan be scheduled. However, if the scale dow n function still fails \\nto enable to high priority job to be scheduled, the scheduler will \\ncall scale up function to maximize the remaining resource that’s \\navailable.  \\n \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.   \\nFig. 2.  Proposed system architecture  A. Weighting Algorithm  \\nOur proposed scheduler utilizes two weighting method.',\n",
       " 'Fig. 2.  Proposed system architecture  A. Weighting Algorithm  \\nOur proposed scheduler utilizes two weighting method. \\nFirst, the scheduler calculates the weight of each job using the \\nresource requirements. Scheduler maximizes resource \\nutilization by prioritizing jobs with high resource requirements \\nfirst. By doing so, scheduler can maximize resource utilization \\nwithout the need of calling scale up function.  \\nSecond, scheduler can prioritize jobs by using a given \\npriority value by the user. This weighting method gives the \\nscheduler flexibility to be ab le to adhere to user’s specifications. \\nScheduler will first prioritize jobs with higher priority value \\nassigned by users, then sort the jobs based on the resource \\nrequirements if two or more jobs have the same priority.  \\nThe weighting algorithm used in our proposed method is \\ndisplayed in (1) and  the notation description is detailed  in Table \\nI. \\n 𝑓(𝛾!,𝛾\")=\\t#!\\n!$\\t&\"# (1) \\nTABLE I.  NOTATION DESCRIPTION FOR WEIGHTING ALGORITHM  \\nNotation Description \\n𝛾! Priority of job [1,100] \\n𝛾\" Decay factor of job [0,1] \\n \\nB. Gang Scheduling Algorithm  \\n Gang scheduling is done by  grouping all pod replicas of a \\njob. The algorithm will calculate the aggregate resource \\nrequirements of all replicas for that job. It will then iterate every \\nnode present in the cluster to find a node that h as the resource \\nFig. 1. Control flow and policy of DRAGON [4]',\n",
       " 'node present in the cluster to find a node that h as the resource \\nFig. 1. Control flow and policy of DRAGON [4] \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.  availability to satisfy the job resource requirements. If there \\nwere no node that can satisfy all replica of the job, then the \\nscheduler won’t force the gang scheduling to be done, but rather \\nscheduling the replicas in the available nodes.  \\nC. Scale Down Function \\nScale down function is used to reduce the number of replicas \\nof a job that’s currently being trained. Scale down function will \\niterate every job that’s currently being trained and tries to reduce \\nits number of replicas. If a job has mor e replica than the \\nminimum number of replicas, then the number of replicas for that job will be reduced.  \\nThe iteration will continue until the available resource can \\nsatisfy the resource requirement of the high priority job. If after \\nall jobs’ replicas ha ve been reduced to its minimum values and \\nthe high priority job still can’t be scheduled, then the scale down function will result in a failure state. If it is successful, the \\nscheduler will reduce the selected jobs’ replicas and schedule the high priority  job. \\nD. Scale Up Function \\nScale up function is used to increase the number of replicas \\nof a job that’s currently being trained. Scale up function will',\n",
       " 'D. Scale Up Function \\nScale up function is used to increase the number of replicas \\nof a job that’s currently being trained. Scale up function will \\niterate every job that’s currently being trained and tries to \\nincrease its number of replicas. If a job has  less replica than the \\nmaximum number of replicas, then the number of replicas for that job will be increased . However, if the available resource \\ncan’t satisfy the addition of replicas, then scale up function won’t be called.  \\nIV. E\\nVALUATION  \\nA. Experimental Environment  \\nThe purpose of our experiments is to measure the training \\nspeed of deep learning jobs using our scheduler, DRAGON, and \\nKubeflow’s default tf -operator. We evaluate both DRAGON \\nand default tf- operator to compare our scheduler’s perfo rmance \\nwith a scheduler from the related works section and also the \\ncontroller Kubeflow use today. To evaluate this, we constructed \\na few experiments with different job specifications to evaluate \\nthe schedulers in different job environments. All experiments \\nare done on Google Kubernetes Engine with 3 nodes. Each node \\nhas 8 vCPU and 30GB Memory, totaling to 24 vCPU and 90 GB \\nMemory in the cluster.  \\nEach experiment scenario is done multiple time to avoid \\nfluke result. The job that was used for our experiments are a \\nlinear regression model using MNIST dataset. Specifications of \\njobs that are modified for each experiment mostly revolves',\n",
       " 'linear regression model using MNIST dataset. Specifications of \\njobs that are modified for each experiment mostly revolves \\naround maximum number of workers, initial number of workers, CPU requirement, Memory requirement, and value of job priority.  \\nFor every job in the experiments, we assume its time \\ncriticality to be not critical. Hence, all jobs in the experiments have a time critical value of 0. This in turn, cause the priority \\nand resource requirements of a job to be the only things that \\naffect the weighting algorithm.  B. Autoscaling and Gang Scheduling Evaluation \\nWe conducted 3 experiments to evaluate the effects of \\nautoscaling and gang scheduling on the training speed of deep learning jobs. Jobs specifications for each experiment is detailed  \\non Table II, Table III, and Table IV.  These experiments are done \\nto compare the training speed of our scheduler,  DRAGON, and \\ntf-operator in three different scenarios.  \\nTABLE II.  JOB SPESIFICATION FOR SCENARIO 1 \\nCategory  Job 1 Job 2 Job 3 Job 4 \\nMaximum worker s  4 6 6 6 \\nMinimum worker s  1 2 1 2 \\nInitial worker replica s 2 2 2 2 \\nCPU requirement  1 2 2 3 \\nMemory requirement  2 Gi 4 Gi 4 Gi 6 Gi \\nPriority 1 1 1 100 \\nTABLE III.  JOB SPESIFICATION FOR SCENARIO 2 \\nCategory  Job 1 Job 2 Job 3 Job 4 \\nMaximum worker s  4 6 6 6 \\nMinimum worker s  1 2 1 2 \\nInitial worker replica s 2 2 2 2 \\nCPU requirement  1 1 1 1 \\nMemory requirement  2 Gi 2 Gi 2 Gi 2 Gi \\nPriority 1 1 1 100 \\nTABLE IV.  JOB SPESIFICATION FOR SCENARIO 3',\n",
       " 'CPU requirement  1 1 1 1 \\nMemory requirement  2 Gi 2 Gi 2 Gi 2 Gi \\nPriority 1 1 1 100 \\nTABLE IV.  JOB SPESIFICATION FOR SCENARIO 3 \\nCategory  Job 1 Job 2 Job 3 Job 4 \\nMaximum worker s  4 6 6 6 \\nMinimum worker s  1 2 1 2 \\nInitial worker replica s 2 2 2 2 \\nCPU requirement  3 3 3 3 \\nMemory requirement  6 Gi 6 Gi 6 Gi 6 Gi \\nPriority 1 1 1 100 \\n \\nC. Weighting Function Evaluation  \\nWe conducted 1 experiment to evaluate the effects of \\nweighting function on the training speed of deep learning jobs. \\nJobs specifications for the experiment is detailed  on Table V.  \\nThis experiment is done to compare schedulers with and without \\nweighting  functionality.\\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.    \\nTABLE V.  JOB SPESIFICATION FOR SCENARIO 4 \\nJob Maximum \\nWorker s Minimum \\nWorker s  Initial \\nWorker s CPU \\nrequirement  Memory  \\nrequirement  Priority \\n1 4 2 4 2 4 1 \\n2 4 2 4 2 4 1 \\n3 4 1 2 2 4 1 \\n4 4 1 2 3 6 1 \\n5 4 2 3 2 4 1 \\n6 4 2 2 2 4 1 \\n7 6 2 2 2 4 1 \\n8 6 1 1 2 4 1 \\n9 4 2 2 2 4 1 \\n10 4 2 2 4 8 1 \\n11 4 3 3 2 4 1 \\n12 4 3 3 2 4 1 \\nD. Model Accuracy Evaluation \\nLastly, we also conducted an experiment to evaluate the \\nmodel generated from distributed deep learning job. Job \\nspecifications for the experiment is detailed on Table VI.  In this \\nexperiment we compare models generated using distributed  \\ndeep learning job and centralized deep learning job.',\n",
       " 'experiment we compare models generated using distributed  \\ndeep learning job and centralized deep learning job.  \\nTABLE VI.  JOB SPESIFICATION FOR SCENARIO 5 \\nVariable  Value  \\nBatch size 100 \\nLearning rate 0.05 \\nGlobal  step 100000 \\n \\nE. Evaluation Result  \\nThe result of autoscaling and gang scheduling, weighting \\nfunction, and model accuracy evaluations are displayed on fig. 3, \\nTable VII, and Table VIII respectively.  \\nEvaluation result of each  scenario shows that for autoscaling \\nand gang scheduling evaluation, on the first scenario, our \\nscheduler resulted in an increase of training speed up to 5.06%. \\nOn the second scenario, our scheduler resulted in an increase of \\ntraining speed up to 35.05%. O n the third scenario, our scheduler \\nresulted in an increase of training speed up to 22.38%.  \\nOn the first experiment, the difference of training speed \\nbetween the three schedulers are not that noticeable. This is \\ncaused due to job 4, the last job in the queue, having a relatively \\nhigh resource requirement. Job 4 most likely can’t be scheduled \\nusing gang scheduling, as other jobs have utilized most of the \\navailable resource. Higher resource requirements also hinder the \\nscale up function, as the scheduler may not be able to scale up \\nfunction successfully even though there are still some available \\nresource left.  On the second experiment, we observed the highest speed \\nincrease between our scheduler, DRAGON, and tf -operator.',\n",
       " 'resource left.  On the second experiment, we observed the highest speed \\nincrease between our scheduler, DRAGON, and tf -operator. \\nLow resource requirement for each job enables the scheduler to \\nmaximally utilize the scale up function to increase the resource \\nutilization when training. Low resource requirements for each \\njob also increase the probability of job replicas to be placed in \\nthe same node, since each node can schedule a handful number \\nof replicas. Unsurprisingly, this experiment also resulted in the \\nfastest training speed for every scheduler in comparison with experiment one and three.  \\nOn the third experiment, even though we have a relatively \\nhigh-speed  increase between our scheduler, DRAGON, and tf-\\noperator, the third experiment resulted in the lowest training \\nspeed for every scheduler in comparison with the previous experiments. This experiment shows that having jobs with high \\nresource requirements will not always improves the training \\nspeed. The communication overhead between parameter server \\nand workers will be higher, resulting an overall slower training \\nspeed.  \\n \\nFig. 3.  Evaluation result for experiment 1, 2, and 3. Lower value is better.  \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.  Evaluation result of the fourth experiment shows that for \\nweighting function evaluation, scheduler with weighting',\n",
       " 'weighting function evaluation, scheduler with weighting \\nfunctionality resulted in an increase of training speed up to \\n6.74% in comparison with a scheduler that has no weighting \\nfunctionality. This shows that prioritizing jobs that has higher \\nresource requirements first results in a faster training speed. This \\nis caused due to schedulers with weighting functionality can \\nmaximize available resource without calling scale up function that case a little bit of overhead in the system.  Evaluation result of the fifth experiment shows that for \\nmodel accuracy, our scheduler still falls behind centralized deep \\nlearning training. The maximum accuracy our scheduler can \\nachieve is merely a 70%. Also, the accuracy seems to be \\nfluctuating and our scheduler still unable to generate a consistent \\nmodel. Th e tradeoff between training speed and model accuracy \\nneeds to be heavily considered, and should be addressed in \\nfurther development for our work\\nTABLE VII.  EVALUATION RESULT FOR EXPERIMENT  4 \\ni Scheduler without \\nWeighting (s) Scheduler with Weighting \\n(s) Speed Increase (%) \\n1 654 564 13.76 \\n2 637 670 -4.93 \\n3 667 680 -1.95 \\n4 671 511 23.85 \\n5 675 673 2.96 \\nMean  660.8 ± 15.47 619.6 ± 77.34 6.74 ± 11.92 \\nTABLE VIII.  EVALUATION RESULT FOR EXPERIMENT  5 \\ni Distributed (%) Centralized (%)  1 worker 2 workers 3 workers 4 workers \\n1 32 51 39 53 91 \\n2 40 28 60 70 90 \\n3 24 62 52 52 90 \\n4 15 57 42 51 91 \\n5 31 38 38 40 92 \\nMean 28.4 ± 9.3 47.2 ± 14 46.2 ± 9.5 53.2 ± 11 90.8 ± 0.8',\n",
       " '1 32 51 39 53 91 \\n2 40 28 60 70 90 \\n3 24 62 52 52 90 \\n4 15 57 42 51 91 \\n5 31 38 38 40 92 \\nMean 28.4 ± 9.3 47.2 ± 14 46.2 ± 9.5 53.2 ± 11 90.8 ± 0.8 \\nV. CONCLUSION  \\nOur proposed method of combining DRAGON’s and \\nOASIS’ approach by creating a scheduler with weighting, \\nautoscaling, and gang scheduling capabilities resulted in an \\nincrease of speed for deep learning training jobs. Evaluation \\ndone using MNIST dataset to each functionality shows that an \\nincrease of up to 26.56% is achieved due to the autoscaling and \\ngang scheduling functionalities, while the weighting algorithm \\nalso resulted in an increase of training speed up to 6.23%.  \\nFurther improvement to our work should be focused on \\nimproving weighting algorithm to also consider the estimation \\nof training time for each job. Improvement on the generated \\nmodel should also be done, so that our scheduler can generate \\nmodels with comparable quality with models generated using centralized deep learning. \\nA\\nCKNOWLEDGEMENT  \\nWe are grateful for receiving funding for publication from \\nthe P3MI program at Institut Teknologi Bandung.  REFERENCES  \\n[1] Kubernetes. (2019). Accessed on 8 Oktober, 2019, from  \\nhttps://www.kubernetes.io/ . \\n[2] Docker. (2019). Accessed on 8 Oktober, 2019, from \\nhttps://www.docker.com/ . \\n[3] Kubeflow. (2019). Accessed on 12 Oktober, 2019, dari https://www.kubeflow.org/ . \\n[4] Lin, C., Yeh, T., dan Chou, J. (2019). DRAGON: A Dynamic Scheduling',\n",
       " '[4] Lin, C., Yeh, T., dan Chou, J. (2019). DRAGON: A Dynamic Scheduling \\nand Scaling Controller for Managing Distributed Deep Learning Jobs in Kubernetes Cluster . DOI:10.5220/00077007605690577.  \\n[5] Bao, Y., Peng, Y. , Wu, C., dan Li, Z. (2018). Online Job Scheduling in \\nDistributed Machine Learning Clusters. CoRR, abs/1801.00936.  \\n[6] Rodriguez, M. dan Buyya, R. (2018). Containers Orchestration with Cost -\\nEfficient Autoscaling in Cloud Computing Environments . CoRR, \\nabs/1812. 00300.  \\n[7] Jeon, M., Venkataraman, S., Phanishayee, A., Qian, J., Xiao, W., dan Yang, F. (2018). Multi -tenant GPU Clusters for Deep Learning \\nWorkloads: Analysis and Implications.  Microsoft Research Technical \\nReport.  \\n[8] Amaral, M., Polo, J., Carrera, D., Seelam, S., dan Steinder, M. (2017). \\nTopology- Aware GPU Scheduling for Learning Workloads in Cloud \\nEnvironments . DOI:10.1145/ 3126908.3126933 . \\n[9] Peteiro -Barral, D. dan Guijarro -Bernidas, B. (2013). A Survey of Methods \\nfor Distributed Machine Learning. DOI:10.1007/s13748 -012-0035- 5.\\n \\nAuthorized licensed use limited to: Intel Corporation via the Virtual Library. Downloaded on November 24,2023 at 16:18:05 UTC from IEEE Xplore.  Restrictions apply.']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and vectorize and store\n",
    "\n",
    "persist_directory = 'basic_langchain/chroma_storage'\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "vectordb_loaded = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm_pipeline,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dragon is a resource scheduler that schedules distributed jobs using gang scheduling and autoscaling, improving upon the scheduling capabilities of Kubernetes.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is dragon?\"\n",
    "result = rag_retrieval.invoke({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline,LlamaForCausalLM,LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 22:29:12 [INFO] Find quantization_config.json, trying to load quantized low bit model...\n",
      "2024-02-27 22:29:12 [INFO] quantization_config: {\n",
      "  \"compute_dtype\": \"fp32\",\n",
      "  \"device\": \"cpu\",\n",
      "  \"low_bit_model\": true,\n",
      "  \"scale_dtype\": \"fp32\",\n",
      "  \"weight_dtype\": \"int8\"\n",
      "}\n",
      "\n",
      "2024-02-27 22:29:12 [INFO] loading weights file /home/ec2-user/mnt/Models/Llama-7b-hf-OPTIM/model.safetensors.index.json\n",
      "2024-02-27 22:29:12 [ERROR] /home/ec2-user/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/libqbits.so: undefined symbol: _ZNR5torch7Library4_defEON3c106eitherINS1_12OperatorNameENS1_14FunctionSchemaEEEONS_11CppFunctionE\n",
      "2024-02-27 22:29:12 [ERROR] Saved low bit model loading failed, please check your model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:240\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mdevice_map\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[1;32m    239\u001b[0m     quantization_config\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(quantization_config, \u001b[39m\"\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 240\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload_low_bit(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    241\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSaved low bit model loading successfully. Other input args \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwill be ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:912\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.load_low_bit\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m     model \u001b[39m=\u001b[39m model_class(config, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 912\u001b[0m model \u001b[39m=\u001b[39m replace_linear(\n\u001b[1;32m    913\u001b[0m     model,\n\u001b[1;32m    914\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mquantization_config,\n\u001b[1;32m    915\u001b[0m     device\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    916\u001b[0m     empty_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    917\u001b[0m )\n\u001b[1;32m    919\u001b[0m \u001b[39mif\u001b[39;00m is_sharded:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/utils.py:68\u001b[0m, in \u001b[0;36mreplace_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, device, empty_weights)\u001b[0m\n\u001b[1;32m     65\u001b[0m     modules_to_not_convert \u001b[39m=\u001b[39m modules_to_not_convert\u001b[39m.\u001b[39mextend(\n\u001b[1;32m     66\u001b[0m         quantization_config\u001b[39m.\u001b[39mllm_int8_skip_modules\n\u001b[1;32m     67\u001b[0m     )\n\u001b[0;32m---> 68\u001b[0m model, is_replaced \u001b[39m=\u001b[39m _replace_linear(\n\u001b[1;32m     69\u001b[0m     model,\n\u001b[1;32m     70\u001b[0m     modules_to_not_convert,\n\u001b[1;32m     71\u001b[0m     current_key_name,\n\u001b[1;32m     72\u001b[0m     quantization_config,\n\u001b[1;32m     73\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     74\u001b[0m     empty_weights\u001b[39m=\u001b[39;49mempty_weights,\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_replaced:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/utils.py:218\u001b[0m, in \u001b[0;36m_replace_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, is_replaced, device, empty_weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_removed \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(module\u001b[39m.\u001b[39mchildren())) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# pylint: disable=E1101\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     _, is_replaced \u001b[39m=\u001b[39m _replace_linear(\n\u001b[1;32m    219\u001b[0m         module,\n\u001b[1;32m    220\u001b[0m         modules_to_not_convert,\n\u001b[1;32m    221\u001b[0m         current_key_name,\n\u001b[1;32m    222\u001b[0m         quantization_config,\n\u001b[1;32m    223\u001b[0m         is_replaced\u001b[39m=\u001b[39;49mis_replaced,\n\u001b[1;32m    224\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    225\u001b[0m         empty_weights\u001b[39m=\u001b[39;49mempty_weights,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[39m# Remove the last key for recursion\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/utils.py:218\u001b[0m, in \u001b[0;36m_replace_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, is_replaced, device, empty_weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_removed \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(module\u001b[39m.\u001b[39mchildren())) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# pylint: disable=E1101\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     _, is_replaced \u001b[39m=\u001b[39m _replace_linear(\n\u001b[1;32m    219\u001b[0m         module,\n\u001b[1;32m    220\u001b[0m         modules_to_not_convert,\n\u001b[1;32m    221\u001b[0m         current_key_name,\n\u001b[1;32m    222\u001b[0m         quantization_config,\n\u001b[1;32m    223\u001b[0m         is_replaced\u001b[39m=\u001b[39;49mis_replaced,\n\u001b[1;32m    224\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    225\u001b[0m         empty_weights\u001b[39m=\u001b[39;49mempty_weights,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[39m# Remove the last key for recursion\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: _replace_linear at line 218 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/utils.py:218\u001b[0m, in \u001b[0;36m_replace_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, is_replaced, device, empty_weights)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_removed \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(module\u001b[39m.\u001b[39mchildren())) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# pylint: disable=E1101\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     _, is_replaced \u001b[39m=\u001b[39m _replace_linear(\n\u001b[1;32m    219\u001b[0m         module,\n\u001b[1;32m    220\u001b[0m         modules_to_not_convert,\n\u001b[1;32m    221\u001b[0m         current_key_name,\n\u001b[1;32m    222\u001b[0m         quantization_config,\n\u001b[1;32m    223\u001b[0m         is_replaced\u001b[39m=\u001b[39;49mis_replaced,\n\u001b[1;32m    224\u001b[0m         device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    225\u001b[0m         empty_weights\u001b[39m=\u001b[39;49mempty_weights,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[39m# Remove the last key for recursion\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/utils.py:118\u001b[0m, in \u001b[0;36m_replace_linear\u001b[0;34m(model, modules_to_not_convert, current_key_name, quantization_config, is_replaced, device, empty_weights)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m device \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m device \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    119\u001b[0m         QuantizedLinearQBits,\n\u001b[1;32m    120\u001b[0m     )  \u001b[39m# TODO: QuantizedLinearINT4, QuantizedLinearINT8\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     model\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m QuantizedLinearQBits(\n\u001b[1;32m    123\u001b[0m         in_features,\n\u001b[1;32m    124\u001b[0m         out_features,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m         scheme\u001b[39m=\u001b[39mquantization_config\u001b[39m.\u001b[39mscheme,\n\u001b[1;32m    132\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/nn/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantizedLinearQBits\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/llm/quantization/nn/modules.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mintel_extension_for_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m matmul_kbit\n\u001b[0;32m---> 29\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(\n\u001b[1;32m     30\u001b[0m     os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mabspath(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(\u001b[39m__file__\u001b[39;49m)), \u001b[39m\"\u001b[39;49m\u001b[39m../../..\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlibqbits.so\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mDropoutQBits_\u001b[39;00m(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mFunction):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_ops.py:933\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m    930\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m    932\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[1;32m    934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/ctypes/__init__.py:376\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[1;32m    377\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: /home/ec2-user/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/libqbits.so: undefined symbol: _ZNR5torch7Library4_defEON3c106eitherINS1_12OperatorNameENS1_14FunctionSchemaEEEONS_11CppFunctionE",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ec2-user/mnt/Rag_demo/RAG/Demo.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224157532d4543322d41492d32227d/home/ec2-user/mnt/Rag_demo/RAG/Demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224157532d4543322d41492d32227d/home/ec2-user/mnt/Rag_demo/RAG/Demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/ec2-user/mnt/Models/Llama-7b-hf-OPTIM\u001b[39m\u001b[39m\"\u001b[39m        \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224157532d4543322d41492d32227d/home/ec2-user/mnt/Rag_demo/RAG/Demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_dir,use_neural_speed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224157532d4543322d41492d32227d/home/ec2-user/mnt/Rag_demo/RAG/Demo.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_dir)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/intel_extension_for_transformers/transformers/modeling/modeling_auto.py:247\u001b[0m, in \u001b[0;36m_BaseQBitsAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m             logger\u001b[39m.\u001b[39merror(e)\n\u001b[1;32m    246\u001b[0m             logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mSaved low bit model loading failed, please check your model.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m             exit(\u001b[39m0\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mintel_extension_for_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_map\u001b[39;00m\n\u001b[1;32m    252\u001b[0m load_in_8bit \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mload_in_8bit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "#Load_model\n",
    "\n",
    "#from intel_extension_for_transformers.transformers import AutoModelForCausalLM\n",
    "##from transformers import AutoTokenizer\n",
    "#model_dir = \"/home/ec2-user/mnt/Models/Llama-7b-hf-OPTIM\"        \n",
    "#model = AutoModelForCausalLM.from_pretrained(model_dir,use_neural_speed=False)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model in memory\n",
    "#Model loaded in memory\n",
    "model_dir = \"/home/ec2-user/mnt/Models/llama-2-7b-chat-hf\"        \n",
    "model = LlamaForCausalLM.from_pretrained(model_dir,ignore_mismatched_sizes=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for LLM\n",
    "pipe= pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, \n",
    "                         trust_remote_code=True, max_new_tokens=100, \n",
    "                         repetition_penalty=1.1, model_kwargs={\"max_length\": 1200, \"temperature\": 0.01})\n",
    "        \n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm_pipeline,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about DRAGON',\n",
       " 'result': \"DRAGON is a scheduler designed by the authors of this paper to reduce resource utilization when system loading is high. Unlike Kubernetes' default scheduler, which uses a First In First Out (FIFO) algorithm, DRAGON uses an Adapted First Come First Served (AFCFS) algorithm. This means that DRAGON prioritizes jobs that come first, but doesn't guarantee that jobs will be scheduled in order. Additionally, D\",\n",
       " 'source_documents': [Document(page_content='function to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization, \\nit allows other jo bs to be scheduled, possibly increasing the \\noverall resource utilization.  \\nGang scheduling is a scheduling method that DRAGON use \\nto reduce communication overhead in the system. Distributed \\ndeep learning divide training process into two entities, i.e. \\nparam eter server and workers. During the training process, \\nparameter server and workers needs to keep exchanging \\ngradients and weights. Gang scheduling is a scheduling \\nalgorithm that schedule parameter server and workers in the \\nsame node in Kubernetes, reducing  the communication \\noverhead needed for parameter server and workers to \\nsynchronize during training. \\nDRAGON, unlike Kubernetes’ default scheduler, uses a n \\nAdapted First Come First Served (AFCFS)  scheduling \\nalgorithm. AFCFS still queue jobs according to its arrival time, \\nbut different from regular First In First Out (FIFO ) scheduling \\nalgorithm, it schedules the first feasible job in the queue. \\nDRAGON prioritizes jobs that come first, but doesn’t guarantee that jobs will be scheduled in order.  \\nDRAGON’s control flow design is detailed on fig. 1. First, \\nscheduler waits until job enters the system. It queues jobs \\naccording to its arrival time, where jobs that arrive first is \\nprioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned'),\n",
       "  Document(page_content='function to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization, \\nit allows other jo bs to be scheduled, possibly increasing the \\noverall resource utilization.  \\nGang scheduling is a scheduling method that DRAGON use \\nto reduce communication overhead in the system. Distributed \\ndeep learning divide training process into two entities, i.e. \\nparam eter server and workers. During the training process, \\nparameter server and workers needs to keep exchanging \\ngradients and weights. Gang scheduling is a scheduling \\nalgorithm that schedule parameter server and workers in the \\nsame node in Kubernetes, reducing  the communication \\noverhead needed for parameter server and workers to \\nsynchronize during training. \\nDRAGON, unlike Kubernetes’ default scheduler, uses a n \\nAdapted First Come First Served (AFCFS)  scheduling \\nalgorithm. AFCFS still queue jobs according to its arrival time, \\nbut different from regular First In First Out (FIFO ) scheduling \\nalgorithm, it schedules the first feasible job in the queue. \\nDRAGON prioritizes jobs that come first, but doesn’t guarantee that jobs will be scheduled in order.  \\nDRAGON’s control flow design is detailed on fig. 1. First, \\nscheduler waits until job enters the system. It queues jobs \\naccording to its arrival time, where jobs that arrive first is \\nprioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned'),\n",
       "  Document(page_content='function to reduce resource utilization when system loading is \\nhigh. Although scale down function reduce resource utilization, \\nit allows other jo bs to be scheduled, possibly increasing the \\noverall resource utilization.  \\nGang scheduling is a scheduling method that DRAGON use \\nto reduce communication overhead in the system. Distributed \\ndeep learning divide training process into two entities, i.e. \\nparam eter server and workers. During the training process, \\nparameter server and workers needs to keep exchanging \\ngradients and weights. Gang scheduling is a scheduling \\nalgorithm that schedule parameter server and workers in the \\nsame node in Kubernetes, reducing  the communication \\noverhead needed for parameter server and workers to \\nsynchronize during training. \\nDRAGON, unlike Kubernetes’ default scheduler, uses a n \\nAdapted First Come First Served (AFCFS)  scheduling \\nalgorithm. AFCFS still queue jobs according to its arrival time, \\nbut different from regular First In First Out (FIFO ) scheduling \\nalgorithm, it schedules the first feasible job in the queue. \\nDRAGON prioritizes jobs that come first, but doesn’t guarantee that jobs will be scheduled in order.  \\nDRAGON’s control flow design is detailed on fig. 1. First, \\nscheduler waits until job enters the system. It queues jobs \\naccording to its arrival time, where jobs that arrive first is \\nprioritized. The scheduler then chec ks whether an urgent job that \\nhas been  waiting inside the queue for more than the assigned')]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_question ='Tell me about DRAGON'\n",
    "\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
