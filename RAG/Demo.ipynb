{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE STEPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model / Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model or OpenAI API, install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download llama from HF\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "#Enter your local directory you want to store the model in\n",
    "save_path = \"Models/Llama-2-7b-hf\"\n",
    "\n",
    "#Specify the model you want to download from HF\n",
    "hf_model = 'meta-llama/Llama-2-7b-hf'\n",
    "access_token='your_token'\n",
    "\n",
    "#Instantiate the model and tokenizer (It downloads weights/architecture/parameters)\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_model, return_dict=True, trust_remote_code=True, token=access_token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\n",
    "#Save the model and the tokenizer in the local directory specified earlier\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Step 1: Load PDF\n",
    "\n",
    "loader = PyPDFLoader(\"/home/ec2-user/mnt/Rag_demo/RAG/Data/Dynamic_Resource_Scheduler_for_Distributed_Deep_Learning_Training_in_Kubernetes.pdf\")\n",
    "pages = loader.load()\n",
    "all_page_text=[p.page_content for p in pages]\n",
    "joined_page_text=\" \".join(all_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it in chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1500,chunk_overlap = 150)\n",
    "splits = text_splitter.split_text(joined_page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and vectorize and store\n",
    "\n",
    "persist_directory = 'basic_langchain/chroma_storage'\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "vectordb.persist()\n",
    "\n",
    "vectordb_loaded = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE : Run the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline,LlamaForCausalLM,LlamaTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=custom_prompt_template,\n",
    "                            input_variables=['context', 'question'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model in memory\n",
    "#Model loaded in memory\n",
    "model_dir = \"/home/ec2-user/mnt/Models/llama-2-7b-chat-hf\"  #DOWNLOADED FROM HUGGING FACE       \n",
    "model = LlamaForCausalLM.from_pretrained(model_dir,ignore_mismatched_sizes=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir,ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for LLM\n",
    "pipe= pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, \n",
    "                         trust_remote_code=True, max_new_tokens=100, \n",
    "                         repetition_penalty=1.1, model_kwargs={\"max_length\": 1200, \"temperature\": 0.01})\n",
    "        \n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm_pipeline,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       #return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now ask a model for \"DRAGON\". This paper proposes a new method called \"dragon\". This is exactly why RAG helps, let's see what happen if we talk to a foundation model about that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_template = \"\"\"Use the following pieces of information to answer the user's question. Explaining the answer\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characteres at maximum please\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "fm_prompt = PromptTemplate.from_template(fm_template)\n",
    "user_question ='Tell me about DRAGON'\n",
    "\n",
    "chain_fm = fm_prompt|llm_pipeline\n",
    "chain_fm.invoke({\"question\": user_question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET ASK TO A RAG MODEL\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Optimized models like : Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/ec2-user/mnt/Models/llama_cpp/llama-2-7b-chat.Q5_K_M.gguf\", # CHANGE IT TO YOUR FOLDER\n",
    "    temperature=0.75,\n",
    "    max_tokens=100,\n",
    "    top_p=1,\n",
    "    #callback_manager=callback_manager,\n",
    "    n_ctx=2048  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG pipeline ( LLM + Retrieval algorithm)\n",
    "rag_retrieval = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                       chain_type='stuff',\n",
    "                                       retriever=vectordb.as_retriever(search_kwargs={'k': 3}),\n",
    "                                       #return_source_documents=True,\n",
    "                                       chain_type_kwargs={'prompt':prompt}\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET ASK TO A RAG MODEL\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
