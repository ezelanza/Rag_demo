{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) Demo\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using LangChain, Hugging Face models, and Chroma vector database. RAG combines the power of retrieval-based and generation-based approaches to provide more accurate and context-aware responses.\n",
    "\n",
    "## What is RAG?\n",
    "RAG is a technique that:\n",
    "1. **Retrieves** relevant documents from a knowledge base\n",
    "2. **Augments** the input prompt with this retrieved context\n",
    "3. **Generates** a response using a language model\n",
    "\n",
    "This approach helps overcome the limitations of pure language models by providing them with specific, relevant information to work with.\n",
    "\n",
    "## Prerequisites and Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model / Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.3.25)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: PyPDF2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (5.29.5)\n",
      "Requirement already satisfied: accelerate in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.7.0)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: peft in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.15.2)\n",
      "Requirement already satisfied: chromadb in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.0.12)\n",
      "Requirement already satisfied: llama-cpp-python in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.3.9)\n",
      "Requirement already satisfied: langchain-community in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.3.25)\n",
      "Requirement already satisfied: pypdf in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (5.6.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.33.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.11.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2.2.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from huggingface_hub->-r requirements.txt (line 14)) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from huggingface_hub->-r requirements.txt (line 14)) (1.1.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.7.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (11.1.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from accelerate->-r requirements.txt (line 7)) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (3.12.12)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.2.2.post1)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (0.34.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (4.9.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.73.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from fastapi==0.115.9->chromadb->-r requirements.txt (line 10)) (0.45.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 11)) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-community->-r requirements.txt (line 12)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-community->-r requirements.txt (line 12)) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-community->-r requirements.txt (line 12)) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 12)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python->-r requirements.txt (line 11)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 10)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 10)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 10)) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (25.2.10)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 10)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 10)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 10)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 10)) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 10)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (3.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 10)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pandas->datasets->-r requirements.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pandas->datasets->-r requirements.txt (line 8)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Download model or OpenAI API, install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Login to Hugging Face Hub (required for accessing some models)\n",
    "# SECURITY: Use environment variables for tokens in production\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Option 1: Use environment variable (recommended)\n",
    "# Set your token in terminal: export HUGGINGFACE_TOKEN=your_token_here\n",
    "hf_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "# Option 2: Manual token input (for development only)\n",
    "if not hf_token:\n",
    "    print(\"⚠️  No HUGGINGFACE_TOKEN environment variable found.\")\n",
    "    print(\"📝 You can either:\")\n",
    "    print(\"   1. Set it as environment variable: export HUGGINGFACE_TOKEN=your_token\")\n",
    "    print(\"   2. Enter it manually below (not recommended for production)\")\n",
    "    # hf_token = input(\"Enter your Hugging Face token: \")  # Uncomment if needed\n",
    "    \n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "    print(\"✅ Successfully logged in to Hugging Face Hub\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping Hugging Face login - some models may not be accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll import all necessary components for our RAG system:\n",
    "- **Document loaders and text splitters** for processing PDFs\n",
    "- **Embedding models** for vector representations\n",
    "- **Vector store** for storing and retrieving documents\n",
    "- **LLM components** for generating responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LlamaCpp support for GGUF models\n"
     ]
    }
   ],
   "source": [
    "# Import all required modules for the RAG system\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline, LlamaCpp  # Added LlamaCpp\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "print(\" LlamaCpp support for GGUF models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Download and Setup Language Model\n",
    "\n",
    "We'll download and set up our language model. In this example, we're using **Llama 3.2 1B**, which is:\n",
    "- Relatively small and fast for demonstration purposes\n",
    "- Good balance between performance and resource requirements\n",
    "- Suitable for local execution\n",
    "\n",
    "The model will be downloaded and saved locally for faster subsequent access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading Llama 8B model in GGUF format...\n",
      "🤖 Model: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\n",
      "📁 File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
      "💾 Saving to: ./llama-models/\n",
      "✅ Model already exists at: ./llama-models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
      "📍 Model path: ./llama-models/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\n",
      "📊 Model size: ~4.9GB (4-bit quantized)\n",
      "🚀 Ready for llama-cpp-python!\n"
     ]
    }
   ],
   "source": [
    "## Download Llama 8B in GGUF format for llama-cpp-python\n",
    "\n",
    "# Configuration for GGUF model download\n",
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Using Llama 3.1 8B in GGUF format (optimized for llama-cpp-python)\n",
    "model_name = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"  # Pre-quantized GGUF model\n",
    "model_file = \"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"    # 4-bit quantized version (~4.9GB)\n",
    "local_dir = \"./llama-models/\"\n",
    "access_token = os.getenv('HUGGINGFACE_TOKEN')  # Uses environment variable for security\n",
    "\n",
    "print(f\"📥 Downloading Llama 8B model in GGUF format...\")\n",
    "print(f\"🤖 Model: {model_name}\")\n",
    "print(f\"📁 File: {model_file}\")\n",
    "print(f\"💾 Saving to: {local_dir}\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Check if model already exists\n",
    "model_path = os.path.join(local_dir, model_file)\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"✅ Model already exists at: {model_path}\")\n",
    "else:\n",
    "    # Download the GGUF model file\n",
    "    print(\"⬇️ Downloading model (this may take a while - ~4.9GB)...\")\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=model_name,\n",
    "        filename=model_file,\n",
    "        local_dir=local_dir,\n",
    "        token=access_token\n",
    "    )\n",
    "    print(f\"✅ Model downloaded successfully!\")\n",
    "\n",
    "print(f\"📍 Model path: {model_path}\")\n",
    "print(f\"📊 Model size: ~4.9GB (4-bit quantized)\")\n",
    "print(\"🚀 Ready for llama-cpp-python!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loading PDF document...\n",
      "✅ Loaded 6 pages\n",
      "📊 Total characters: 24,094\n",
      "📝 First 200 characters: 978-1-7281-8038-0/20/$31.00 ©2020 IEEE \n",
      " \n",
      "Dynamic Resource Scheduler for Distributed Deep \n",
      "Learning Training in Kubernetes \n",
      "Muhammad Fadhriga Bestari \n",
      "School of Electrical Engineering and Informatics,...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load PDF Document\n",
    "print(\"📄 Loading PDF document...\")\n",
    "\n",
    "# Load the PDF file using PyPDFLoader\n",
    "pdf_path = \"./Data/Dynamic_Resource_Scheduler_for_Distributed_Deep_Learning_Training_in_Kubernetes.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# Extract text content from all pages\n",
    "all_page_text = [p.page_content for p in pages]\n",
    "joined_page_text = \" \".join(all_page_text)\n",
    "\n",
    "print(f\"✅ Loaded {len(pages)} pages\")\n",
    "print(f\"📊 Total characters: {len(joined_page_text):,}\")\n",
    "print(f\"📝 First 200 characters: {joined_page_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Splitting text into chunks...\n",
      "✅ Created 29 text chunks\n",
      "📏 Average chunk size: 947 characters\n",
      "\n",
      "📖 First chunk preview:\n",
      "978-1-7281-8038-0/20/$31.00 ©2020 IEEE \n",
      " \n",
      "Dynamic Resource Scheduler for Distributed Deep \n",
      "Learning Training in Kubernetes \n",
      "Muhammad Fadhriga Bestari \n",
      "School of Electrical Engineering and Informatics, ITB, \n",
      "Indonesia \n",
      "fadhriga.bestari@gmail.com \n",
      " \n",
      "Achmad Imam Kistijantoro1,2 \n",
      "1School of Electrical E...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'978-1-7281-8038-0/20/$31.00 ©2020 IEEE \\n \\nDynamic Resource Scheduler for Distributed Deep \\nLearning Training in Kubernetes \\nMuhammad Fadhriga Bestari \\nSchool of Electrical Engineering and Informatics, ITB, \\nIndonesia \\nfadhriga.bestari@gmail.com \\n \\nAchmad Imam Kistijantoro1,2 \\n1School of Electrical Engineering and Informatics, ITB, \\nIndonesia \\n2University Center of Excellence on Artificial Intelligence \\nfor Vision, Natural Language Processing & Big Data \\nAnalytics (U-CoE AI-VLB), Indonesia \\nimam@stei.itb.ac.id\\n \\nAnggrahita Bayu Sasmita \\nSchool of Electrical Engineering and Informatics, ITB, Indonesia \\nangga@stei.itb.ac.id \\n \\n \\nAbstract—Distributed deep learning is a method of machine \\nlearning that is used today due to its many advantages. One of the \\nmany tools used to train distributed deep learning model is \\nKubeflow, which runs on top of Kubernetes. Kubernetes is a \\ncontainerized application orchestrator that ease the deploy ment'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Split text into chunks\n",
    "print(\"✂️ Splitting text into chunks...\")\n",
    "\n",
    "# Configure the text splitter\n",
    "# chunk_size: Maximum characters per chunk\n",
    "# chunk_overlap: Characters to overlap between chunks (maintains context)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,    # Optimal size for most embedding models\n",
    "    chunk_overlap=150   # Overlap to maintain context between chunks\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_text(joined_page_text)\n",
    "\n",
    "print(f\"✅ Created {len(splits)} text chunks\")\n",
    "print(f\"📏 Average chunk size: {sum(len(chunk) for chunk in splits) // len(splits)} characters\")\n",
    "print(f\"\\n📖 First chunk preview:\\n{splits[0][:300]}...\")\n",
    "\n",
    "# Display the first chunk as output\n",
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Creating embeddings and vector store...\n",
      "🤖 Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "💾 Vector store location: chroma_vectorstore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_23612/3186929147.py:20: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector store created with 29 documents\n",
      "🎯 Ready for similarity search and retrieval!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_23612/3186929147.py:31: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n",
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_23612/3186929147.py:34: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb_loaded = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create embeddings and vector store\n",
    "print(\"🔢 Creating embeddings and vector store...\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set up the embedding model and storage directory\n",
    "persist_directory = 'chroma_vectorstore'\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"🤖 Using embedding model: {embedding_model}\")\n",
    "print(f\"💾 Vector store location: {persist_directory}\")\n",
    "\n",
    "# Clean up any existing vector store to avoid conflicts\n",
    "if os.path.exists(persist_directory):\n",
    "    print(f\"🧹 Cleaning up existing vector store...\")\n",
    "    shutil.rmtree(persist_directory)\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "# Create vector store from text chunks\n",
    "# This process converts each text chunk into a numerical vector\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Persist the vector store to disk for future use\n",
    "vectordb.persist()\n",
    "\n",
    "# Load the persisted vector store (demonstrates how to reload)\n",
    "vectordb_loaded = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "print(f\"✅ Vector store created with {len(splits)} documents\")\n",
    "print(\"🎯 Ready for similarity search and retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct LLM prompt created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a simple prompt template for direct LLM queries\n",
    "direct_llm_template = \"\"\"You are a helpful AI assistant. Answer the following question to the best of your knowledge in 1000 characters at maximum.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Create prompt template\n",
    "direct_prompt = PromptTemplate(\n",
    "    template=direct_llm_template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# Create a simple function to query LLM directly\n",
    "def query_llm_directly(question):\n",
    "    \"\"\"Query the LLM directly without any document context\"\"\"\n",
    "    formatted_prompt = direct_prompt.format(question=question)\n",
    "    response = llm_cpp(formatted_prompt)\n",
    "    return response\n",
    "\n",
    "print(\"Direct LLM prompt created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG prompt created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the RAG prompt template\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful AI assistant. Answer the question using ONLY the provided context. Give a concise answer in maximum 100 words. Do NOT repeat information.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Direct answer:\"\"\"\n",
    "\n",
    "# Create the improved prompt template object\n",
    "RAG_prompt_template = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")\n",
    "\n",
    "print(\"RAG prompt created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Loading the Language Model\n",
    "\n",
    "Now we'll load our pre-downloaded language model and create a text generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Loading Llama 8B with llama-cpp-python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Llama 8B loaded successfully with llama-cpp-python!\n"
     ]
    }
   ],
   "source": [
    "## Load Llama 8B with llama-cpp-python\n",
    "\n",
    "print(\"🤖 Loading Llama 8B with llama-cpp-python...\")\n",
    "\n",
    "# Set up callback manager for streaming output (optional)\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "\n",
    "# Configure LlamaCpp with BETTER parameters to prevent repetition\n",
    "llm_cpp = LlamaCpp(\n",
    "    model_path=model_path,           \n",
    "    n_ctx=2048,                      # Context window size\n",
    "    n_batch=512,                     # Batch size for processing\n",
    "    n_threads=8,                     # Number of CPU threads\n",
    "    max_tokens=150,                  # 150 for complete responses\n",
    "    temperature=0.1,                 # 0.1 for more focused responses\n",
    "    top_p=0.8,                       # 0.8 for less randomness\n",
    "    repeat_penalty=2.0,              # 2.0 to prevent repetition\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,                   \n",
    "    streaming=True,                  \n",
    "    stop=[\"Question:\", \"Context:\", \"\\n\\n\", \"Note:\", \"Answer:\", \"Human:\", \"Assistant:\", \"fig.\", \"design is\"]  # More stop sequences\n",
    ")\n",
    "\n",
    "# Update the global llm_cpp variable\n",
    "llm_cpp = llm_cpp\n",
    "\n",
    "print(\"✅ Llama 8B loaded successfully with llama-cpp-python!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Creating the Complete RAG Pipeline\n",
    "\n",
    "Now we'll combine everything into a working RAG system:\n",
    "1. **Text Generation Pipeline**: Configures how the model generates text\n",
    "2. **LangChain Integration**: Wraps the pipeline for use with LangChain\n",
    "3. **Retrieval QA Chain**: Combines retrieval and generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Creating RAG retrieval chain with Llama 8B...\n",
      "✅ RAG pipeline with Llama 8B created successfully!\n",
      "🎯 Configuration:\n",
      "  - Model: Llama 3.1 8B Instruct (4-bit quantized)\n",
      "  - Backend: llama-cpp-python (optimized)\n",
      "  - Retrieval: Top 3 most similar documents\n",
      "  - Context: 2048 tokens\n",
      "  - Chain type: Stuff (concatenate all retrieved docs)\n",
      "  - ✨ Streaming: Enabled for both direct LLM and RAG!\n",
      "💡 This should be much faster and use less memory than transformers!\n"
     ]
    }
   ],
   "source": [
    "## Create RAG Pipeline with Llama 8B (llama-cpp-python)\n",
    "\n",
    "print(\"🔗 Creating RAG retrieval chain with Llama 8B...\")\n",
    "\n",
    "# Create the complete RAG pipeline using llama-cpp-python with streaming support\n",
    "rag_retrieval_cpp = RetrievalQA.from_chain_type(\n",
    "    llm=llm_cpp,                                           # Using LlamaCpp instead of HuggingFacePipeline\n",
    "    chain_type='stuff',                                    # How to combine retrieved docs\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 3}),  # Retrieve top 3 similar docs\n",
    "    chain_type_kwargs={'prompt': RAG_prompt_template},                  # Use our custom prompt\n",
    "    return_source_documents=False                          # Don't return source docs for cleaner output\n",
    ")\n",
    "\n",
    "# Create a streaming function for RAG\n",
    "def query_rag_streaming(question):\n",
    "    \"\"\"Query RAG with streaming support\"\"\"\n",
    "    print(\"📝 RAG Answer: \", end=\"\", flush=True)\n",
    "    # Use invoke instead of run for better control\n",
    "    result = rag_retrieval_cpp.invoke({\"query\": question})\n",
    "    return result['result'] if 'result' in result else result\n",
    "\n",
    "print(\"✅ RAG pipeline with Llama 8B created successfully!\")\n",
    "print(\"🎯 Configuration:\")\n",
    "print(\"  - Model: Llama 3.1 8B Instruct (4-bit quantized)\")\n",
    "print(\"  - Backend: llama-cpp-python (optimized)\")\n",
    "print(\"  - Retrieval: Top 3 most similar documents\")\n",
    "print(\"  - Context: 2048 tokens\")\n",
    "print(\"  - Chain type: Stuff (concatenate all retrieved docs)\")\n",
    "print(\"  - ✨ Streaming: Enabled for both direct LLM and RAG!\")\n",
    "print(\"💡 This should be much faster and use less memory than transformers!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Comparing RAG vs Base Model\n",
    "\n",
    "Now let's test our RAG system and compare it with the base model to see the difference!\n",
    "\n",
    "## The Test Case: \"DRAGON\"\n",
    "We'll ask about \"DRAGON\" - a specific method mentioned in our PDF document. This is a perfect example of why RAG is useful:\n",
    "\n",
    "- **Base Model**: Will likely give generic information about dragons (mythical creatures)\n",
    "- **RAG Model**: Should provide specific information about the DRAGON method from the research paper\n",
    "\n",
    "This comparison will clearly demonstrate RAG's ability to provide contextually relevant, document-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Testing Direct LLM vs RAG Comparison\n",
      "============================================================\n",
      "❓ Test Question: What is DRAGON?\n",
      "\n",
      "==============================\n",
      "🤖 1. DIRECT LLM RESPONSE (No Document Context):\n",
      "----------------------------------------\n",
      "💭 Model will use only its training knowledge...\n",
      " DRAGON is a type of astronomical object, specifically an active galactic nucleus (AGN). It's characterized by extremely high energy output and strong emission lines. The name \"DRAGO\" comes from the acronym for its spectral characteristics: D = Dust; R= Radio continuum ; A=GALACTIC NUCLEUS EMISSION LINES.\n",
      "\n",
      "🧠 2. RAG LLM RESPONSE (With Document Context):\n",
      "----------------------------------------\n",
      "📚 Model will use retrieved document context...\n",
      "📝 RAG Answer:  DRAGON is a resource scheduler that schedules distributed jobs using gang scheduling and autoscaling.  (Maximum of around ~70 words) "
     ]
    }
   ],
   "source": [
    "# Test Direct LLM vs RAG Comparison\n",
    "print(\"🔄 Testing Direct LLM vs RAG Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test question\n",
    "test_question = \"What is DRAGON?\"\n",
    "\n",
    "print(f\"❓ Test Question: {test_question}\")\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "\n",
    "# 1. Test Direct LLM (No Context)\n",
    "print(\"🤖 1. DIRECT LLM RESPONSE (No Document Context):\")\n",
    "print(\"-\" * 40)\n",
    "print(\"💭 Model will use only its training knowledge...\")\n",
    "\n",
    "direct_response = query_llm_directly(test_question)\n",
    "\n",
    "# 2. Test RAG LLM (With Context)\n",
    "print(\"\\n\")\n",
    "print(\"🧠 2. RAG LLM RESPONSE (With Document Context):\")\n",
    "print(\"-\" * 40)\n",
    "print(\"📚 Model will use retrieved document context...\")\n",
    "\n",
    "# Use the new streaming RAG function\n",
    "rag_response = query_rag_streaming(test_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully built and tested a complete RAG system. Here's what we accomplished:\n",
    "\n",
    "### ✅ What We Built\n",
    "1. **Document Processing Pipeline**: Loaded and chunked a PDF document\n",
    "2. **Vector Store**: Created embeddings and stored them in Chroma\n",
    "3. **Language Model Integration**: Set up Llama 3.2 1B for text generation\n",
    "4. **RAG Pipeline**: Combined retrieval and generation for context-aware responses\n",
    "\n",
    "### 🎯 Key Benefits Demonstrated\n",
    "- **Contextual Accuracy**: RAG provides specific, document-relevant information\n",
    "- **Knowledge Grounding**: Responses are based on actual document content\n",
    "- **Reduced Hallucination**: Less likely to generate incorrect information\n",
    "\n",
    "### 🚀 Potential Improvements\n",
    "1. **Multiple Documents**: Add more PDFs to expand the knowledge base\n",
    "2. **Better Chunking**: Experiment with different chunk sizes and overlap\n",
    "3. **Advanced Retrieval**: Try different similarity search methods\n",
    "4. **Larger Models**: Use more powerful language models for better responses\n",
    "5. **Evaluation Metrics**: Add quantitative evaluation of response quality\n",
    "\n",
    "### 💡 Use Cases\n",
    "This RAG system can be adapted for:\n",
    "- **Research Assistant**: Query academic papers and documents\n",
    "- **Customer Support**: Answer questions based on product documentation\n",
    "- **Legal Research**: Search through legal documents and cases\n",
    "- **Technical Documentation**: Query API docs, manuals, and guides\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opea-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
