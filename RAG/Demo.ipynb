{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation) Demo\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using LangChain, Hugging Face models, and Chroma vector database. RAG combines the power of retrieval-based and generation-based approaches to provide more accurate and context-aware responses.\n",
    "\n",
    "## What is RAG?\n",
    "RAG is a technique that:\n",
    "1. **Retrieves** relevant documents from a knowledge base\n",
    "2. **Augments** the input prompt with this retrieved context\n",
    "3. **Generates** a response using a language model\n",
    "\n",
    "This approach helps overcome the limitations of pure language models by providing them with specific, relevant information to work with.\n",
    "\n",
    "## Prerequisites and Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model / Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.3.25)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.52.4)\n",
      "Requirement already satisfied: sentence-transformers in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (4.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: PyPDF2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (3.0.1)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (5.29.5)\n",
      "Requirement already satisfied: accelerate in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.7.0)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: peft in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.15.2)\n",
      "Requirement already satisfied: chromadb in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.0.12)\n",
      "Requirement already satisfied: llama-cpp-python in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.3.9)\n",
      "Requirement already satisfied: langchain-community in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.3.25)\n",
      "Requirement already satisfied: pypdf in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (5.6.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.33.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.3.65)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.11.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2.2.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from transformers->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from huggingface_hub->-r requirements.txt (line 14)) (2025.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from huggingface_hub->-r requirements.txt (line 14)) (1.1.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.7.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (1.15.3)\n",
      "Requirement already satisfied: Pillow in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 3)) (11.1.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from accelerate->-r requirements.txt (line 7)) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from datasets->-r requirements.txt (line 8)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (3.12.12)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.2.2.post1)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (0.34.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (4.9.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (1.73.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from chromadb->-r requirements.txt (line 10)) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from fastapi==0.115.9->chromadb->-r requirements.txt (line 10)) (0.45.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 11)) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-community->-r requirements.txt (line 12)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-community->-r requirements.txt (line 12)) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from langchain-community->-r requirements.txt (line 12)) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 8)) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 12)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 12)) (1.1.0)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python->-r requirements.txt (line 11)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 10)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 10)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 10)) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 10)) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (25.2.10)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 10)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 10)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 10)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 10)) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.55b1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (0.55b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.55b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 10)) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 10)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 10)) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 10)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 10)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 3)) (3.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 10)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 10)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 10)) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 10)) (10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pandas->datasets->-r requirements.txt (line 8)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from pandas->datasets->-r requirements.txt (line 8)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/anaconda3/envs/opea-env/lib/python3.11/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 3)) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Download model or OpenAI API, install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Login to Hugging Face Hub (required for accessing some models)\n",
    "# Replace with your own token or use environment variables for security\n",
    "from huggingface_hub import login\n",
    "login(token = 'your_hugging_face_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll import all necessary components for our RAG system:\n",
    "- **Document loaders and text splitters** for processing PDFs\n",
    "- **Embedding models** for vector representations\n",
    "- **Vector store** for storing and retrieving documents\n",
    "- **LLM components** for generating responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required modules for the RAG system\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline, LlamaForCausalLM, LlamaTokenizer\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Download and Setup Language Model\n",
    "\n",
    "We'll download and set up our language model. In this example, we're using **Llama 3.2 1B**, which is:\n",
    "- Relatively small and fast for demonstration purposes\n",
    "- Good balance between performance and resource requirements\n",
    "- Suitable for local execution\n",
    "\n",
    "The model will be downloaded and saved locally for faster subsequent access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading model: meta-llama/Llama-3.2-1B\n",
      "üíæ Saving to: meta-llama/Llama-3.2-1B\n",
      "‚úÖ Model downloaded and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Configuration for model download and storage\n",
    "save_path = \"meta-llama/Llama-3.2-1B\"  # Local directory to store the model\n",
    "hf_model = 'meta-llama/Llama-3.2-1B'   # Hugging Face model identifier\n",
    "access_token = 'your_token'             # Replace with your HF token\n",
    "\n",
    "print(f\"üì• Downloading model: {hf_model}\")\n",
    "print(f\"üíæ Saving to: {save_path}\")\n",
    "\n",
    "# Download model and tokenizer from Hugging Face\n",
    "# This downloads the model weights, architecture, and configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    hf_model, \n",
    "    return_dict=True, \n",
    "    trust_remote_code=True, \n",
    "    token=access_token\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model)\n",
    "\n",
    "# Save the model and tokenizer locally for future use\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"‚úÖ Model downloaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading PDF document...\n",
      "‚úÖ Loaded 6 pages\n",
      "üìä Total characters: 24,094\n",
      "üìù First 200 characters: 978-1-7281-8038-0/20/$31.00 ¬©2020 IEEE \n",
      " \n",
      "Dynamic Resource Scheduler for Distributed Deep \n",
      "Learning Training in Kubernetes \n",
      "Muhammad Fadhriga Bestari \n",
      "School of Electrical Engineering and Informatics,...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load PDF Document\n",
    "print(\"üìÑ Loading PDF document...\")\n",
    "\n",
    "# Load the PDF file using PyPDFLoader\n",
    "pdf_path = \"./Data/Dynamic_Resource_Scheduler_for_Distributed_Deep_Learning_Training_in_Kubernetes.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load()\n",
    "\n",
    "# Extract text content from all pages\n",
    "all_page_text = [p.page_content for p in pages]\n",
    "joined_page_text = \" \".join(all_page_text)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(pages)} pages\")\n",
    "print(f\"üìä Total characters: {len(joined_page_text):,}\")\n",
    "print(f\"üìù First 200 characters: {joined_page_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Splitting text into chunks...\n",
      "‚úÖ Created 18 text chunks\n",
      "üìè Average chunk size: 1450 characters\n",
      "\n",
      "üìñ First chunk preview:\n",
      "978-1-7281-8038-0/20/$31.00 ¬©2020 IEEE \n",
      " \n",
      "Dynamic Resource Scheduler for Distributed Deep \n",
      "Learning Training in Kubernetes \n",
      "Muhammad Fadhriga Bestari \n",
      "School of Electrical Engineering and Informatics, ITB, \n",
      "Indonesia \n",
      "fadhriga.bestari@gmail.com \n",
      " \n",
      "Achmad Imam Kistijantoro1,2 \n",
      "1School of Electrical E...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'978-1-7281-8038-0/20/$31.00 ¬©2020 IEEE \\n \\nDynamic Resource Scheduler for Distributed Deep \\nLearning Training in Kubernetes \\nMuhammad Fadhriga Bestari \\nSchool of Electrical Engineering and Informatics, ITB, \\nIndonesia \\nfadhriga.bestari@gmail.com \\n \\nAchmad Imam Kistijantoro1,2 \\n1School of Electrical Engineering and Informatics, ITB, \\nIndonesia \\n2University Center of Excellence on Artificial Intelligence \\nfor Vision, Natural Language Processing & Big Data \\nAnalytics (U-CoE AI-VLB), Indonesia \\nimam@stei.itb.ac.id\\n \\nAnggrahita Bayu Sasmita \\nSchool of Electrical Engineering and Informatics, ITB, Indonesia \\nangga@stei.itb.ac.id \\n \\n \\nAbstract‚ÄîDistributed deep learning is a method of machine \\nlearning that is used today due to its many advantages. One of the \\nmany tools used to train distributed deep learning model is \\nKubeflow, which runs on top of Kubernetes. Kubernetes is a \\ncontainerized application orchestrator that ease the deploy ment \\nprocess of applications. This in turn makes distributed deep \\nlearning training done in Kubeflow easier and manageable. Works \\non a dynamic resource scheduler in Kubernetes for deep learning \\ntraining have been done before, such as DRAGON that proposed \\nscheduler with autoscaling and gang scheduling capabilities, and \\nOASIS that proposed a utility system with price function. In this \\nwork, we propose to combine DRAGON‚Äôs and OASIS‚Äô approach \\nto make a scheduler with weighted autoscaling capabilities and'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Split text into chunks\n",
    "print(\"‚úÇÔ∏è Splitting text into chunks...\")\n",
    "\n",
    "# Configure the text splitter\n",
    "# chunk_size: Maximum characters per chunk\n",
    "# chunk_overlap: Characters to overlap between chunks (maintains context)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,    # Optimal size for most embedding models\n",
    "    chunk_overlap=150   # Overlap to maintain context between chunks\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_text(joined_page_text)\n",
    "\n",
    "print(f\"‚úÖ Created {len(splits)} text chunks\")\n",
    "print(f\"üìè Average chunk size: {sum(len(chunk) for chunk in splits) // len(splits)} characters\")\n",
    "print(f\"\\nüìñ First chunk preview:\\n{splits[0][:300]}...\")\n",
    "\n",
    "# Display the first chunk as output\n",
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Creating embeddings and vector store...\n",
      "ü§ñ Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üíæ Vector store location: basic_langchain/chroma_storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_8436/3902327617.py:12: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created with 18 documents\n",
      "üéØ Ready for similarity search and retrieval!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_8436/3902327617.py:23: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n",
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_8436/3902327617.py:26: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb_loaded = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create embeddings and vector store\n",
    "print(\"üî¢ Creating embeddings and vector store...\")\n",
    "\n",
    "# Set up the embedding model and storage directory\n",
    "persist_directory = 'basic_langchain/chroma_storage'\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "print(f\"ü§ñ Using embedding model: {embedding_model}\")\n",
    "print(f\"üíæ Vector store location: {persist_directory}\")\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "# Create vector store from text chunks\n",
    "# This process converts each text chunk into a numerical vector\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=splits,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# Persist the vector store to disk for future use\n",
    "vectordb.persist()\n",
    "\n",
    "# Load the persisted vector store (demonstrates how to reload)\n",
    "vectordb_loaded = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {len(splits)} documents\")\n",
    "print(\"üéØ Ready for similarity search and retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USAGE : Run the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Creating prompt template...\n",
      "‚úÖ Prompt template created successfully!\n",
      "\n",
      "üìã Template structure:\n",
      "- Context: Retrieved relevant documents\n",
      "- Question: User's query\n",
      "- Instructions: How to format the response\n"
     ]
    }
   ],
   "source": [
    "# Define the RAG prompt template\n",
    "print(\"üìù Creating prompt template...\")\n",
    "\n",
    "custom_prompt_template = \"\"\"Use the following pieces of information to answer the user's question. Explain the answer clearly.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characters at maximum please.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template object\n",
    "prompt = PromptTemplate(\n",
    "    template=custom_prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Prompt template created successfully!\")\n",
    "print(\"\\nüìã Template structure:\")\n",
    "print(\"- Context: Retrieved relevant documents\")\n",
    "print(\"- Question: User's query\")\n",
    "print(\"- Instructions: How to format the response\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Loading the Language Model\n",
    "\n",
    "Now we'll load our pre-downloaded language model and create a text generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Loading language model...\n",
      "üìÇ Loading from: ./meta-llama/Llama-3.2-1B/\n",
      "‚úÖ Model loaded successfully!\n",
      "üîß Model type: LlamaForCausalLM\n",
      "üìä Model parameters: ~1.2B\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-downloaded model from local directory\n",
    "print(\"ü§ñ Loading language model...\")\n",
    "\n",
    "model_dir = \"./meta-llama/Llama-3.2-1B/\"  # Path to our downloaded model\n",
    "print(f\"üìÇ Loading from: {model_dir}\")\n",
    "\n",
    "# Load the tokenizer and model from local storage\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, ignore_mismatched_sizes=True)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üîß Model type: {type(model).__name__}\")\n",
    "print(f\"üìä Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e9:.1f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Creating the Complete RAG Pipeline\n",
    "\n",
    "Now we'll combine everything into a working RAG system:\n",
    "1. **Text Generation Pipeline**: Configures how the model generates text\n",
    "2. **LangChain Integration**: Wraps the pipeline for use with LangChain\n",
    "3. **Retrieval QA Chain**: Combines retrieval and generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Setting up text generation pipeline...\n",
      "üîó Creating RAG retrieval chain...\n",
      "‚úÖ RAG pipeline created successfully!\n",
      "üéØ Configuration:\n",
      "  - Retrieval: Top 3 most similar documents\n",
      "  - Generation: Llama 3.2 1B with low temperature\n",
      "  - Chain type: Stuff (concatenate all retrieved docs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/1h25ypjx3qlgy0kwpr9s61qm0000gn/T/ipykernel_8436/630915865.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "# Create the text generation pipeline\n",
    "print(\"‚öôÔ∏è Setting up text generation pipeline...\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    max_new_tokens=100,           # Maximum tokens to generate\n",
    "    repetition_penalty=1.1,       # Reduce repetitive text\n",
    "    model_kwargs={\n",
    "        \"max_length\": 1200,       # Maximum total sequence length\n",
    "        \"temperature\": 0.01       # Low temperature for more focused responses\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap the pipeline for LangChain compatibility\n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(\"üîó Creating RAG retrieval chain...\")\n",
    "\n",
    "# Create the complete RAG pipeline\n",
    "rag_retrieval = RetrievalQA.from_chain_type(\n",
    "    llm=llm_pipeline,\n",
    "    chain_type='stuff',                                    # How to combine retrieved docs\n",
    "    retriever=vectordb.as_retriever(search_kwargs={'k': 3}),  # Retrieve top 3 similar docs\n",
    "    chain_type_kwargs={'prompt': prompt}                   # Use our custom prompt\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG pipeline created successfully!\")\n",
    "print(\"üéØ Configuration:\")\n",
    "print(\"  - Retrieval: Top 3 most similar documents\")\n",
    "print(\"  - Generation: Llama 3.2 1B with low temperature\")\n",
    "print(\"  - Chain type: Stuff (concatenate all retrieved docs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Comparing RAG vs Base Model\n",
    "\n",
    "Now let's test our RAG system and compare it with the base model to see the difference!\n",
    "\n",
    "## The Test Case: \"DRAGON\"\n",
    "We'll ask about \"DRAGON\" - a specific method mentioned in our PDF document. This is a perfect example of why RAG is useful:\n",
    "\n",
    "- **Base Model**: Will likely give generic information about dragons (mythical creatures)\n",
    "- **RAG Model**: Should provide specific information about the DRAGON method from the research paper\n",
    "\n",
    "This comparison will clearly demonstrate RAG's ability to provide contextually relevant, document-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Base Model (without context)\n",
      "==================================================\n",
      "‚ùì Question: Tell me about DRAGON\n",
      "\n",
      "ü§ñ Base Model Response:\n",
      "üìù Dragon is a game where you can play with your friends by playing together online or locally.\n",
      "You can also play other games like \"PewDiePie\", \"Fortnite\",and many more.\n",
      "There are different modes such as Free for All, Team Deathmatch, Capture the Flag etc. You can also buy in-game items using real money (in-app purchases).\n",
      "The game has various gameplay features such as auto-aim, infinite ammo, unlimited health, no recoil, no ADS etc.\n",
      "It\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Base Model (without RAG context)\n",
    "print(\"üîç Testing Base Model (without context)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple prompt template for the base model\n",
    "fm_template = \"\"\"Use the following pieces of information to answer the user's question. Explain the answer clearly.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else. Give an answer in 1000 characters at maximum please.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "fm_prompt = PromptTemplate.from_template(fm_template)\n",
    "user_question = 'Tell me about DRAGON'\n",
    "\n",
    "print(f\"‚ùì Question: {user_question}\")\n",
    "print(\"\\nü§ñ Base Model Response:\")\n",
    "\n",
    "# Create chain and get response\n",
    "chain_fm = fm_prompt | llm_pipeline\n",
    "response = chain_fm.invoke({\"question\": user_question})\n",
    "\n",
    "# Extract and display the answer\n",
    "answer_only = response.split(\"answer:\")[-1].strip()\n",
    "print(f\"üìù {answer_only}\")\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing RAG Model (with context)\n",
      "==================================================\n",
      "‚ùì Question: Tell me about DRAGON\n",
      "\n",
      "üéØ RAG Model Response:\n",
      "üìù DRAGON is a distributed deep learning framework developed by Google Research team. DRAGON provides a general programming model for training and executing large-scale neural networks with thousands of parameters, called DAG network. This paper is about the DAG network in detail.\n",
      "\n",
      "==================================================\n",
      "üéâ Comparison Complete!\n",
      "\n",
      "üìä Key Differences:\n",
      "‚Ä¢ Base Model: Provides general knowledge about dragons\n",
      "‚Ä¢ RAG Model: Provides specific information from the research paper\n",
      "‚Ä¢ RAG demonstrates how retrieval enhances generation with relevant context\n"
     ]
    }
   ],
   "source": [
    "# Test 2: RAG Model (with retrieved context)\n",
    "print(\"üîç Testing RAG Model (with context)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"‚ùì Question: {user_question}\")\n",
    "print(\"\\nüéØ RAG Model Response:\")\n",
    "\n",
    "# Get response from RAG system\n",
    "response = rag_retrieval.invoke({\"query\": user_question})\n",
    "rag_answer = response[\"result\"].split(\"answer:\")[-1].strip()\n",
    "print(f\"üìù {rag_answer}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ Comparison Complete!\")\n",
    "print(\"\\nüìä Key Differences:\")\n",
    "print(\"‚Ä¢ Base Model: Provides general knowledge about dragons\")\n",
    "print(\"‚Ä¢ RAG Model: Provides specific information from the research paper\")\n",
    "print(\"‚Ä¢ RAG demonstrates how retrieval enhances generation with relevant context\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You've successfully built and tested a complete RAG system. Here's what we accomplished:\n",
    "\n",
    "### ‚úÖ What We Built\n",
    "1. **Document Processing Pipeline**: Loaded and chunked a PDF document\n",
    "2. **Vector Store**: Created embeddings and stored them in Chroma\n",
    "3. **Language Model Integration**: Set up Llama 3.2 1B for text generation\n",
    "4. **RAG Pipeline**: Combined retrieval and generation for context-aware responses\n",
    "\n",
    "### üéØ Key Benefits Demonstrated\n",
    "- **Contextual Accuracy**: RAG provides specific, document-relevant information\n",
    "- **Knowledge Grounding**: Responses are based on actual document content\n",
    "- **Reduced Hallucination**: Less likely to generate incorrect information\n",
    "\n",
    "### üöÄ Potential Improvements\n",
    "1. **Multiple Documents**: Add more PDFs to expand the knowledge base\n",
    "2. **Better Chunking**: Experiment with different chunk sizes and overlap\n",
    "3. **Advanced Retrieval**: Try different similarity search methods\n",
    "4. **Larger Models**: Use more powerful language models for better responses\n",
    "5. **Evaluation Metrics**: Add quantitative evaluation of response quality\n",
    "\n",
    "### üí° Use Cases\n",
    "This RAG system can be adapted for:\n",
    "- **Research Assistant**: Query academic papers and documents\n",
    "- **Customer Support**: Answer questions based on product documentation\n",
    "- **Legal Research**: Search through legal documents and cases\n",
    "- **Technical Documentation**: Query API docs, manuals, and guides\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opea-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
